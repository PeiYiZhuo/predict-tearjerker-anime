{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94c2286",
   "metadata": {},
   "source": [
    "# When Computers Cry: Predicting Tearjerker Anime Using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0797fa3",
   "metadata": {},
   "source": [
    "One of the few pieces of media to have ever coaxed tears from me is Kyoto Animation's devastating 2008 anime series *Clannad: After Story*, a show that I enthusiatically recommend. And I do so not in spite of the fact that it crushed my soul but because of it. If anything, I consider an anime's ability to turn on the waterworks to be a marker of quality. Most of my favorite anime are considered tearjerkers, *A Place Further than the Universe*, *Your Name*, and *K-On* come to mind as notable examples. Even if I don't tear up while watching them, a decent attempt made by an anime's creators still gets enormous credit from me. Given my afinity for such shows, wouldn't it be great if I could somehow predict whether an anime will trigger this emotional reaction from the audience? How well would I be able to do so?\n",
    "\n",
    "Using data scraped off of the website MyAnimeList courtesy of , here's what I did . . .\n",
    "\n",
    "1. Determined for anime whether each is a tearjerker or not based on the frequency of key words, \"cry\", \"tears,\" etc., in their reviews.\n",
    "2. Used the synopsis of the anime from MyAnimeList along with to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "# https://pub.towardsai.net/emoticon-and-emoji-in-text-mining-7392c49f596a\n",
    "# https://medium.com/geekculture/text-preprocessing-how-to-handle-emoji-emoticon-641bbfa6e9e7\n",
    "from emot.emo_unicode import EMOTICONS_EMO, EMOJI_UNICODE\n",
    "# https://www.kaggle.com/code/shsagar/finding-anime-genre-based-on-synopsis-logistic-reg\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd399243",
   "metadata": {},
   "source": [
    "# Wrangle Anime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews?select=animes.csv\n",
    "animes = pd.read_csv(\"data/animes.csv\") \n",
    "# https://www.kaggle.com/datasets/azathoth42/myanimelist?select=AnimeList.csv\n",
    "AnimeList = pd.read_csv(\"data/AnimeList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnimeList = AnimeList[[\"anime_id\", \n",
    "                       \"title_english\", \n",
    "                       \"title_synonyms\", \n",
    "                       \"type\", \n",
    "                       \"source\", \n",
    "                       \"producer\", \n",
    "                       \"licensor\", \n",
    "                       \"studio\"]]\n",
    "animes = animes.merge(AnimeList, how = \"left\", left_on = \"uid\", right_on = \"anime_id\")\n",
    "anime = animes[[\"uid\", \n",
    "                \"title\", \n",
    "                \"score\",\n",
    "                \"synopsis\", \n",
    "                \"genre\", \n",
    "                \"type\",\n",
    "                \"episodes\", \n",
    "                \"source\", \n",
    "                \"producer\", \n",
    "                \"licensor\", \n",
    "                \"studio\"]]\n",
    "anime = anime.drop_duplicates()\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d55bdc",
   "metadata": {},
   "source": [
    "# Construct Target Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff2e23",
   "metadata": {},
   "source": [
    "My target field comes from a dataset of anime reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.read_csv(\"data/reviews.csv\")\n",
    "# reviews = reviews.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3335c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# total_rows = len(reviews)\n",
    "# max_rows = 10000\n",
    "# num_files = math.ceil(total_rows/max_rows)\n",
    "\n",
    "# start = 0\n",
    "# end = 9999\n",
    "\n",
    "# for i in range(1, num_files + 1):\n",
    "#     i = str(i)\n",
    "#     print(\"Writing file #\" + i)\n",
    "#     reviews.iloc[start:(end + 1), :].to_csv(\"data/reviews/reviews\" + i + \".csv\", index = False)\n",
    "#     start += 10000\n",
    "#     end += 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8840c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "reviews = pd.read_csv(\"data/reviews/reviews1.csv\")\n",
    "for i in range(2, 15):\n",
    "    i = str(i)\n",
    "    print(\"Concatenating data file #\" + i)\n",
    "    addition = pd.read_csv(\"data/reviews/reviews\" + i + \".csv\")\n",
    "    reviews = pd.concat([reviews, addition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant fields\n",
    "reviews = reviews[[\"uid\", \"anime_uid\", \"link\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counts = reviews.groupby(\"anime_uid\")[\"anime_uid\"].count() \n",
    "keep_anime = review_counts[review_counts >= 10].index.tolist()\n",
    "reviews = reviews[reviews[\"anime_uid\"].isin(keep_anime)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481d3f7",
   "metadata": {},
   "source": [
    "## Remove front and back matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba92f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.reset_index(drop = True)\n",
    "filler = re.compile(r\"^[\\s\\w]*Enjoyment[\\s\\d]*|\\s*Helpful\\s*$\")\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(filler, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00402ae3",
   "metadata": {},
   "source": [
    "## Drop titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b8b93",
   "metadata": {},
   "source": [
    "I intend to use the frequency of key words such as \"cry\" and \"tears\" in user reviews to determine whether an anime is a tearjerker or not. Some anime include these key words in their titles, meaning users really can't help but mention them in their reviews. Thus, to ensure that I'll only be counting authentic appearances of these terms, I exclude from the text of each review the title of the anime being discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5706d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.merge(anime[[\"uid\", \"title\"]], how = \"left\", left_on = \"anime_uid\", right_on = \"uid\")\n",
    "reviews = reviews.drop(columns = \"uid_y\").rename(columns = {\"uid_x\": \"uid\"})\n",
    "reviews[\"text\"] = reviews[\"text\"].str.lower()\n",
    "reviews[\"title\"] = reviews[\"title\"].str.lower()\n",
    "\n",
    "for i in range(len(reviews)):\n",
    "    print(\"Processing row #\" + str(i))\n",
    "    title = reviews[\"title\"].iloc[i]\n",
    "    bigrams = reviews[\"title\"].iloc[[i]].str.split().iloc[0]\n",
    "    # http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "    bigrams = [\" \".join(bigram) for bigram in list(zip(bigrams, bigrams[1:]))]\n",
    "    pattern = \"|\".join([re.escape(title)] + [re.escape(bigram) for bigram in bigrams])\n",
    "    reviews[\"text\"].iloc[[i]] = reviews[\"text\"].iloc[[i]].str.replace(pattern, \"\", regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66452f7d",
   "metadata": {},
   "source": [
    "## Test `TweetTokenizer()` from NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_emoticons.drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39132ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_emoticons[used_emoticons.iloc[:, 0] == \"T.T\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"].iloc[1148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2020/11/text-cleaning-nltk-library/\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet = TweetTokenizer()\n",
    "tweet.tokenize(reviews[\"text\"].iloc[1148])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea346fe",
   "metadata": {},
   "source": [
    "## Replace emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e197e",
   "metadata": {},
   "source": [
    "The reviews feature heavy use of emoticons. These symbols allow their users to succintly communicate an emotional reaction through pictograms comprised of punctuation, letters, numbers, etc. Because they convey information on emotion, I want to retain them to help me determine whether an anime is a tearjerker or not. However, since they include punctuation, which will be removed from the text later on in the process of constructing the target field, I opted to replace them with verbal descriptions.\n",
    "\n",
    "I used a dictionary of emoticons from the emo_unicode library . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba493fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://introtopython.org/dictionaries.html#General-Syntax\n",
    "emoticons = {}\n",
    "for symbol, meaning in EMOTICONS_EMO.items():\n",
    "    emoticons[symbol] = \"\".join([word.capitalize() for word in meaning.replace(\",\", \"\").split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ea7",
   "metadata": {},
   "source": [
    ". . . and added a few missing emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons[\"(^—^)\"] = \"NormalLaugh\"\n",
    "# https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "emoticons[\">W<\"] = \"Troubled\"\n",
    "emoticons[\"-_-'\"] = \"Troubled\"\n",
    "# -_-“ 325386\n",
    "# :s)\n",
    "# https://www.urbandictionary.com/define.php?term=%3E_%3E\n",
    "emoticons[\">_>\"] = \"RightSidewaysLook\"\n",
    "emoticons[\"<_<\"] = \"LeftSidewaysLook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5788f9a8",
   "metadata": {},
   "source": [
    "Since many long emoticons are simply short ones with additional characters, I sorted the emoticons in descending order according to their length so that, later on, the longer emoticons would be matched prior to shorter emoticons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "# https://www.w3schools.com/python/ref_func_sorted.asp\n",
    "emoticons = dict(sorted(emoticons.items(), key = lambda item: -len(item[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5895d",
   "metadata": {},
   "source": [
    "I then assembled a list of the emoticons from the dictionary that were used in the reviews. I managed to get , but I'm sure I missed some T_T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pythonforbeginners.com/basics/list-comprehensions-in-python\n",
    "# https://www.geeksforgeeks.org/python-accessing-key-value-in-dictionary/\n",
    "# https://stackoverflow.com/questions/4202538/escape-special-characters-in-a-python-string\n",
    "pattern = \"\\s(\" + \"|\".join([re.escape(emoticon) for emoticon in emoticons]) + \")\\W?\"\n",
    "used_emoticons = reviews[\"text\"].str.extractall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f32092",
   "metadata": {},
   "source": [
    "Here, I'm converting the dataframe of used emoticons into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/python-convert-numpy-array-to-list\n",
    "used_emoticons = used_emoticons.dropna().drop_duplicates().values.reshape(1, -1)[0].tolist()\n",
    "# https://stackoverflow.com/questions/5352546/extract-subset-of-key-value-pairs-from-dictionary\n",
    "used_emoticons = {symbol: emoticons[symbol] for symbol in used_emoticons}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f858c4b",
   "metadata": {},
   "source": [
    "Again, I'm sorting the emoticons according to their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_emoticons = dict(sorted(used_emoticons.items(), key = lambda item: -len(item[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a26df",
   "metadata": {},
   "source": [
    "Now, I'm looping through each emoticon and replacing it with its respective value in the dictionary if it's preceded by a white space and followed by a non-word character. These conditions are to reduce the likelihood of false-positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef79d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, meaning in used_emoticons.items():\n",
    "    print(\"Replacing \" + symbol + \" with \" + meaning)\n",
    "    reviews[\"text\"] = reviews[\"text\"].str.replace(\"(?<=\\s)\" + re.escape(symbol) + \"(?=\\W?)\", meaning, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.to_csv(\"data/intermediates/replace_emoticons.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/replace_emoticons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531148e",
   "metadata": {},
   "source": [
    "## Replace emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {}\n",
    "for meaning, symbol in EMOJI_UNICODE.items():\n",
    "    emojis[symbol] = \"_\".join(meaning.lower().replace(\",\", \"\").replace(\":\", \"\").split())\n",
    "emojis['❤️'] = \"heart\"\n",
    "emojis['♥️'] = \"heart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13cb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"(\" + \"|\".join([re.escape(emoji) for emoji in emojis]) + \")\"\n",
    "used_emojis = reviews[\"text\"].str.extractall(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"].iloc[[19803]].str.contains('☺')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d57ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews[\"link\"].iloc[19803])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_emojis = used_emojis.dropna().drop_duplicates().values.reshape(1, -1)[0].tolist()\n",
    "# used_emojis = {symbol: emojis[symbol] for symbol in used_emojis}\n",
    "# for symbol, meaning in used_emojis.items():\n",
    "#     print(\"Replacing \" + symbol + \" with \" + meaning)\n",
    "#     reviews[\"text\"] = reviews[\"text\"].str.replace(symbol, \" \" + meaning + \" \", regex = False)\n",
    "used_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da981203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_characters = reviews[\"text\"].str.extract(r\"(?P<final_character>.$)\", expand = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a1c17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "final_characters = reviews[\"text\"].str.extract(r\"(?P<final_character>.$)\")\n",
    "counts = final_characters.groupby(\"final_character\")[\"final_character\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(counts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ce3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"][final_characters['final_character'] == '️']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[counts.index == '️']\n",
    "# '⠀', '️', '̿'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b58543",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.strip()\n",
    "reviews[\"text\"] = reviews[\"text\"].str.lower()\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"\\\\\", \" \", regex = True)\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"/\", \" \")                                      \n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"‘\", \"'\").str.replace(\"’\", \"'\")\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"“\", '\"').str.replace(\"”\", '\"')\n",
    "pattern = \"[\" + string.punctuation.replace(\"'\", \"\").replace(\"-\", \"\") + \"–\" + \"…\" + \"]\" \n",
    "pattern = pattern + r\"|(?<=\\s)'(?=\\w)|(?<=\\w)'(?=\\s)\"\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(pattern, \"\", regex = True)\n",
    "reviews[\"text\"] = reviews[\"text\"].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.explode(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555994d",
   "metadata": {},
   "source": [
    "## Replace non-word characters except for emojis and select punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca51a32",
   "metadata": {},
   "source": [
    "I will drop all non-word characters that aren't emojis. First, I create a list of the unique non-word characters present in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c24573",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = reviews[\"text\"].drop_duplicates()\n",
    "non_word = unique_tokens.str.extractall(r\"(\\W)\").drop_duplicates()\n",
    "non_word = non_word.values.reshape(1, -1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571ea57",
   "metadata": {},
   "source": [
    "Next, I reformat the definitions from the emoji dictionary that I obtained from the emot library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a68511",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {}\n",
    "for meaning, symbol in EMOJI_UNICODE.items():\n",
    "    emojis[symbol] = meaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a6804",
   "metadata": {},
   "source": [
    "I created a regex pattern that separates all the characters that I want to be dropped by a pipe, `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51976328/best-way-to-remove-xad-in-python\n",
    "# https://stackoverflow.com/questions/31522361/python-getting-rid-of-u200b-from-a-string-using-regular-expressions\n",
    "# https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "drop = [char for char in non_word if char not in emojis]\n",
    "drop.remove(\"'\")\n",
    "drop.remove(\"-\")\n",
    "drop = \"|\".join(drop) # Create regex pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62892f82",
   "metadata": {},
   "source": [
    "Using the pattern created above, I replaced the non-word characters I specified with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.replace(drop, \"\", regex = True)\n",
    "reviews = reviews[reviews[\"text\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews[\"text\"].loc[168].values\n",
    "# reviews[\"text\"].loc[281].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf0568",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e89917",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords = list(stopwords.words(\"English\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[~reviews[\"text\"].isin(stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fefbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.to_csv(\"data/intermediates/cleaned_text.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/cleaned_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5f175",
   "metadata": {},
   "source": [
    "## Add target field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_words = [\"sad\",\n",
    "             \"saddest\",\n",
    "             \"emotion\",\n",
    "             \"emotions\",\n",
    "             \"emotional\",\n",
    "             \"emotionally\",\n",
    "             \"depressed\",\n",
    "             \"depressing\",\n",
    "             \"depressingly\",\n",
    "             \"tragic\",\n",
    "             \"tragedy\"]\n",
    "\n",
    "cry_words = [\"cry\", \n",
    "             \"cried\", \n",
    "             \"crying\", \n",
    "             \"sob\", \n",
    "             \"sobbed\", \n",
    "             \"sobbing\", \n",
    "             \"bawl\", \n",
    "             \"bawled\", \n",
    "             \"bawling\", \n",
    "             \"tear\", \n",
    "             \"tears\", \n",
    "             \"teared\", \n",
    "             \"tearing\", # as in \"tearing up\"\n",
    "             \"sadorcrying\",\n",
    "             \"tearsofhappiness\",\n",
    "             \"sadofcrying\",\n",
    "             \"😭\"]\n",
    "\n",
    "reviews[\"sad\"] = reviews[\"text\"].isin(sad_words) \n",
    "reviews[\"cry\"] = reviews[\"text\"].isin(cry_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_vote = reviews.groupby([\"uid\", \"anime_uid\"])[[\"sad\", \"cry\"]].sum()\n",
    "cry_vote[\"sad_cry\"] = (cry_vote[\"sad\"] > 0) & (cry_vote[\"cry\"] > 0)\n",
    "cry_vote = cry_vote[\"sad_cry\"].reset_index()\n",
    "cry_vote = cry_vote.groupby(\"anime_uid\")[\"sad_cry\"].mean().reset_index()\n",
    "cry_vote = cry_vote.rename(columns = {\"sad_cry\": \"cry_vote\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_vote.to_csv(\"data/intermediates/cry_vote.csv\", index = False)\n",
    "cry_vote = pd.read_csv(\"data/intermediates/cry_vote.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed377fdc",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930885e",
   "metadata": {},
   "source": [
    "*True Tears* is a peculiar case because one of its characters' ability to cry is a central plot point. Thus, reviewers who are summarizing its story mention crying quite a lot even after scrubbing its reviews of any mention of the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c217d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = cry_vote.merge(anime, how = \"left\", left_on = \"anime_uid\", right_on = \"uid\").drop(columns = \"anime_uid\")\n",
    "anime[[\"title\", \"uid\", \"cry_vote\"]].sort_values(\"cry_vote\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec186b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "anime[\"cry_vote\"].hist(bins = 30, edgecolor = \"black\")\n",
    "plt.ylim(bottom = 0, top = 1250)\n",
    "plt.xlabel('Proportion of reviews that mention \"sad crying\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(anime[\"cry_vote\"] > 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime[\"cry_vote\"] = anime[\"cry_vote\"] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e997c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.to_csv(\"data/intermediates/target_added.csv\", index = False)\n",
    "anime = pd.read_csv(\"data/intermediates/target_added.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d77be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.groupby(\"cry_vote\")[\"episodes\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30f777",
   "metadata": {},
   "source": [
    "# Clean final data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = anime[anime[\"title\"] != \"True Tears\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = anime[[\n",
    "               \"cry_vote\" \n",
    "               , \"score\"\n",
    "#              , \"episodes\"\n",
    "               , \"synopsis\"\n",
    "               , \"genre\" \n",
    "#              , \"studio\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248db5f1",
   "metadata": {},
   "source": [
    "## Drop anime with missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = anime.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee976b",
   "metadata": {},
   "source": [
    "## Clean `genre` and `studio` for `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca55720",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime[\"genre\"] = anime[\"genre\"].str.replace(\"[\\[\\]']\", \"\", regex = True).str.split(\", \")\n",
    "anime[\"genre\"] = anime[\"genre\"].apply(lambda genres: [genre.replace(\" \", \"\") for genre in genres])\n",
    "anime[\"genre\"] = anime[\"genre\"].apply(lambda genres: \" \".join(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime[\"studio\"] = anime[\"studio\"].str.split(\", \")\n",
    "# anime[\"studio\"] = anime[\"studio\"].apply(lambda studios: [studio.replace(\" \", \"\") for studio in studios])\n",
    "# anime[\"studio\"] = anime[\"studio\"].apply(lambda studios: \" \".join(studios))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de3435",
   "metadata": {},
   "source": [
    "## Clean `synopsis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607431c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = re.compile(r\"(?<=\\n)[^\\n]*$\")\n",
    "has_filler = anime[\"synopsis\"].str.extractall(\"(.)$\").iloc[:, 0].str.match(\"[\\]\\)]\").reset_index(drop = True)\n",
    "anime[\"synopsis\"][has_filler] = anime[\"synopsis\"][has_filler].str.replace(filler, \"\").str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7b96d",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac45cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(anime.drop(columns = \"cry_vote\"), \n",
    "                                                    anime[\"cry_vote\"], \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282cce2f",
   "metadata": {},
   "source": [
    "# Data transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b976f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    (\"Normalize episodes\", StandardScaler(), [\"score\"])\n",
    "#   , (\"Normalize episodes\", StandardScaler(), [\"episodes\"])\n",
    "    # https://stackoverflow.com/questions/65242617/sklearn-pipeline-with-countvectorizer-and-category-on-a-pandas-dataframe\n",
    "    # https://stackoverflow.com/questions/58772181/columntransformer-fails-with-countvectorizer-in-a-pipeline\n",
    "    , (\"Extract features from synopsis\", CountVectorizer(stop_words = stopwords), \"synopsis\")\n",
    "    , (\"Extract features from genre\", CountVectorizer(stop_words = stopwords), \"genre\")\n",
    "#   , (\"Extract features from studio\", CountVectorizer(stop_words = stopwords), \"studio\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_test = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ec865",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf2d7a",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c601e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification\n",
    "# pred = model.predict_proba(X_test)[:, 1] > 0.48\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc815db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa2429",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfa0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9239d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a713f7a",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8370177",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f400aea",
   "metadata": {},
   "source": [
    "# Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ccb67",
   "metadata": {},
   "source": [
    "These projects were especially helpful in helping me create my own.\n",
    "\n",
    "- [Finding Anime Genre Based on Synopsis(Logistic Reg](https://www.kaggle.com/code/shsagar/finding-anime-genre-based-on-synopsis-logistic-reg)\n",
    "- [A Beginner's Guide to Sentiment Analysis with Python](https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6)\n",
    "\n",
    "I also relied on this [regular expression cheat sheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
