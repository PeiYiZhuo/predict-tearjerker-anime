{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94c2286",
   "metadata": {},
   "source": [
    "# When Computers Cry: Predicting Tearjerker Anime Using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0797fa3",
   "metadata": {},
   "source": [
    "One of the few pieces of media to have ever coaxed tears from me is Kyoto Animation's devastating 2008 anime series *Clannad: After Story*, a show that I enthusiatically recommend. And I do so not in spite of the fact that it crushed my soul but because of it. If anything, I consider an anime's ability to turn on the waterworks to be a marker of quality. Most of my favorite anime are considered tearjerkers, *A Place Further than the Universe*, *Your Name*, and *K-On* come to mind as notable examples. Even if I don't tear up while watching them, a decent attempt made by an anime's creators still gets enormous credit from me. Given my interest in anime, I decided to take on the task of predicting tearjerker anime as a starter project for me to clarify and advance my understanding of machine learning and Scikit-learn.\n",
    "\n",
    "Using data scraped off of the website MyAnimeList courtesy of , here's what I did . . .\n",
    "\n",
    "1. Determined for anime whether each is a tearjerker or not based on the frequency of key words, \"cry\", \"tears,\" etc., in their reviews.\n",
    "2. Used the synopsis of the anime from MyAnimeList along with to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "# https://pub.towardsai.net/emoticon-and-emoji-in-text-mining-7392c49f596a\n",
    "# https://medium.com/geekculture/text-preprocessing-how-to-handle-emoji-emoticon-641bbfa6e9e7\n",
    "from emot.emo_unicode import EMOTICONS_EMO, EMOJI_UNICODE\n",
    "# https://www.kaggle.com/code/shsagar/finding-anime-genre-based-on-synopsis-logistic-reg\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd399243",
   "metadata": {},
   "source": [
    "# Wrangle Anime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews?select=animes.csv\n",
    "animes = pd.read_csv(\"data/animes.csv\") \n",
    "# https://www.kaggle.com/datasets/azathoth42/myanimelist?select=AnimeList.csv\n",
    "AnimeList = pd.read_csv(\"data/AnimeList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnimeList = AnimeList[[\"anime_id\", \n",
    "                       \"title_english\", \n",
    "                       \"title_synonyms\", \n",
    "                       \"type\", \n",
    "                       \"source\", \n",
    "                       \"producer\", \n",
    "                       \"licensor\", \n",
    "                       \"studio\"]]\n",
    "animes = animes.merge(AnimeList, how = \"left\", left_on = \"uid\", right_on = \"anime_id\")\n",
    "anime = animes[[\"uid\", \n",
    "                \"title\", \n",
    "                \"title_english\", \n",
    "                \"title_synonyms\",\n",
    "                \"score\",\n",
    "                \"members\",\n",
    "                \"type\",\n",
    "                \"episodes\",\n",
    "                \"synopsis\", \n",
    "                \"genre\", \n",
    "                \"source\", \n",
    "                \"studio\",\n",
    "                \"producer\", \n",
    "                \"licensor\"]]\n",
    "anime = anime.drop_duplicates()\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d55bdc",
   "metadata": {},
   "source": [
    "# What is a tearjerker anime?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff2e23",
   "metadata": {},
   "source": [
    "In order to predict tearjerker anime, I must define it. To construct this target vector, I used a dataset of anime reviews. Thus, the determination of whether an anime is a tearjerker or not is based upon reviewers' reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.read_csv(\"data/reviews.csv\")\n",
    "# reviews = reviews.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3335c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# total_rows = len(reviews)\n",
    "# max_rows = 10000\n",
    "# num_files = math.ceil(total_rows/max_rows)\n",
    "\n",
    "# start = 0\n",
    "# end = 9999\n",
    "\n",
    "# for i in range(1, num_files + 1):\n",
    "#     i = str(i)\n",
    "#     print(\"Writing file #\" + i)\n",
    "#     reviews.iloc[start:(end + 1), :].to_csv(\"data/reviews/reviews\" + i + \".csv\", index = False)\n",
    "#     start += 10000\n",
    "#     end += 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8840c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "reviews = pd.read_csv(\"data/reviews/reviews1.csv\")\n",
    "for i in range(2, 15):\n",
    "    i = str(i)\n",
    "    print(f\"Concatenating data file #{i}\")\n",
    "    addition = pd.read_csv(f\"data/reviews/reviews{i}.csv\")\n",
    "    reviews = pd.concat([reviews, addition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant fields\n",
    "reviews = reviews[[\"uid\", \"anime_uid\", \"link\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counts = reviews.groupby(\"anime_uid\")[\"anime_uid\"].count() \n",
    "keep_anime = review_counts[review_counts >= 10].index.tolist()\n",
    "reviews = reviews[reviews[\"anime_uid\"].isin(keep_anime)]\n",
    "reviews = reviews.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24216acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = reviews.groupby(\"anime_uid\")[\"uid\"].nunique().rename(\"n\")\n",
    "# reviews = reviews.sort_values([\"anime_uid\", \"uid\"]).groupby(\"anime_uid\").head(1)\n",
    "# reviews = reviews.merge(n, how = \"left\", left_on = \"anime_uid\", right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481d3f7",
   "metadata": {},
   "source": [
    "## Remove front and back matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba92f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = re.compile(r\"^[\\s\\w]*Enjoyment[\\s\\d]*|\\s*Helpful\\s*$\") # Assumes reviews never start with a number\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(filler, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea346fe",
   "metadata": {},
   "source": [
    "## Replace emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e197e",
   "metadata": {},
   "source": [
    "The reviews feature heavy use of emoticons. These symbols allow users to succintly communicate an emotional reaction through pictograms comprised of punctuation, letters, numbers, etc. Because they convey information on emotion, I want to retain them to help me determine whether an anime is a tearjerker or not. However, since they include punctuation, which will be removed from the text later on in the process of constructing the target vector, I opted to replace them with verbal descriptions.\n",
    "\n",
    "I used a dictionary of emoticons from the `emot` library ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba493fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://introtopython.org/dictionaries.html#General-Syntax\n",
    "emoticons = {}\n",
    "for symbol, meaning in EMOTICONS_EMO.items():\n",
    "    emoticons[symbol] = \"\".join([word.capitalize() for word in meaning.replace(\",\", \"\").split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ea7",
   "metadata": {},
   "source": [
    "... and added a few missing emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons[\"(^‚Äî^)\"] = \"NormalLaugh\"\n",
    "emoticons[\"-_-‚Äú\"] = \"Troubled\" \n",
    "emoticons[\":s)\"] = \"HappyFaceOrSmiley\" \n",
    "emoticons[\":S)\"] = \"HappyFaceOrSmiley\" \n",
    "# https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "emoticons[\">W<\"] = \"Troubled\"\n",
    "emoticons[\"-_-'\"] = \"Troubled\"\n",
    "# https://www.urbandictionary.com/define.php?term=%3E_%3E\n",
    "emoticons[\">_>\"] = \"RightSidewaysLook\"\n",
    "emoticons[\"<_<\"] = \"LeftSidewaysLook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5788f9a8",
   "metadata": {},
   "source": [
    "Since many long emoticons are simply short ones with additional characters, I sorted the emoticons in descending order according to their length so that, later on, the longer emoticons would be matched prior to shorter emoticons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "# https://www.w3schools.com/python/ref_func_sorted.asp\n",
    "emoticons = dict(sorted(emoticons.items(), key = lambda item: -len(item[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5895d",
   "metadata": {},
   "source": [
    "I then assembled a list of emoticons that feature in both the dictionary and the reviews. I managed to get 97 unique emoticons, but I'm sure I missed some T_T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pythonforbeginners.com/basics/list-comprehensions-in-python\n",
    "# https://www.geeksforgeeks.org/python-accessing-key-value-in-dictionary/\n",
    "# https://stackoverflow.com/questions/4202538/escape-special-characters-in-a-python-string\n",
    "pattern = \"\\s(\" + \"|\".join([re.escape(emoticon) for emoticon in emoticons]) + \")\\W?\"\n",
    "used_emoticons = reviews[\"text\"].str.extractall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f32092",
   "metadata": {},
   "source": [
    "Here, I'm converting the dataframe of used emoticons into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/python-convert-numpy-array-to-list\n",
    "used_emoticons = used_emoticons.dropna().drop_duplicates().iloc[:, 0].tolist()\n",
    "# https://stackoverflow.com/questions/5352546/extract-subset-of-key-value-pairs-from-dictionary\n",
    "used_emoticons = {symbol: emoticons[symbol] for symbol in used_emoticons}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f858c4b",
   "metadata": {},
   "source": [
    "Again, I'm sorting the emoticons according to their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_emoticons = dict(sorted(used_emoticons.items(), key = lambda item: -len(item[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a26df",
   "metadata": {},
   "source": [
    "Now, I'm looping through each emoticon and replacing it with its respective value in the dictionary if it's preceded by a white space and followed by a non-word character. These conditions are meant to reduce the likelihood of false-positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef79d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, meaning in used_emoticons.items():\n",
    "    print(f\"Replacing {symbol} with {meaning}\")\n",
    "    reviews[\"text\"] = reviews[\"text\"].str.replace(\"(?<=\\s)\" + re.escape(symbol) + \"(?=\\W?)\", meaning, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv(\"data/intermediates/replace_emoticons.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/replace_emoticons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f142f",
   "metadata": {},
   "source": [
    "## Drop titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca14a82",
   "metadata": {},
   "source": [
    "I use the appearance of key words such as \"cry\" and \"tears\" in user reviews to determine whether an anime is a tearjerker or not. Some anime include these key words in their titles, meaning users really can't help but mention them in their reviews. Thus, to ensure that I'll only be counting authentic appearances of these terms, I exclude the title of the anime being discussed from the text of each review.\n",
    "\n",
    "*True Tears* is a peculiar case because one of its characters' ability to cry is a central plot point. Thus, reviewers who are summarizing its story mention crying quite a lot even after scrubbing its reviews of any mention of the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03eec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.merge(\n",
    "    anime[[\"uid\", \"title\", \"title_english\", \"title_synonyms\"]], \n",
    "    how = \"left\", left_on = \"anime_uid\", right_on = \"uid\"\n",
    ")\n",
    "reviews = reviews.drop(columns = \"uid_y\").rename(columns = {\"uid_x\": \"uid\"})\n",
    "reviews[\"text\"] = reviews[\"text\"].str.lower()\n",
    "reviews[\"title\"] = reviews[\"title\"].str.lower()\n",
    "reviews[\"title_english\"] = reviews[\"title_english\"].str.lower()\n",
    "reviews[\"title_synonyms\"] = reviews[\"title_synonyms\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in [\"title\", \"title_english\", \"title_synonyms\"]:\n",
    "    for i in range(len(reviews)):\n",
    "        print(f\"Processing row #{str(i)} for {field}\")\n",
    "        title = reviews[field].iloc[i]\n",
    "        if not pd.isna(title):\n",
    "            unigrams = reviews[field].iloc[[i]].str.split().iloc[0]\n",
    "            # http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "            bigrams = [\" \".join(unigram) for unigram in list(zip(unigrams, unigrams[1:]))]\n",
    "            pattern = \"|\".join([re.escape(title)] + [re.escape(bigram) for bigram in bigrams])\n",
    "            reviews[\"text\"].iloc[[i]] = reviews[\"text\"].iloc[[i]].str.replace(pattern, \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6247fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop(columns = [\"title\", \"title_english\", \"title_synonyms\"])\n",
    "reviews.to_csv(\"data/intermediates/drop_titles.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/drop_titles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b58543",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e591f",
   "metadata": {},
   "source": [
    "I turn the table of reviews where each row is one review into a table of tokens where each row is one token from a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.strip()\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"\\\\\", \" \", regex = True)\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"/\", \" \")                                      \n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"‚Äò\", \"'\").str.replace(\"‚Äô\", \"'\")\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"‚Äú\", '\"').str.replace(\"‚Äù\", '\"')\n",
    "# \"‚Äì\" is used as punctuation while \"-\" is used to create phrases\n",
    "pattern = \"[\" + string.punctuation.replace(\"'\", \"\").replace(\"-\", \"\") + \"‚Äì\" + \"‚Ä¶\" + \"]\" \n",
    "pattern = pattern + r\"|(?<=\\s)'(?=\\w)|(?<=\\w)'(?=\\s)\"\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(pattern, \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.split()\n",
    "reviews = reviews.explode(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv(\"data/intermediates/tokenize_text.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/tokenize_text.csv\", na_filter = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555994d",
   "metadata": {},
   "source": [
    "## Replace non-word characters except for emojis and select punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca51a32",
   "metadata": {},
   "source": [
    "To further cleanse the tokens, I want to drop all non-word characters that aren't emojis. For the same reason as emoticons, I want to use emojis to create my target vector because they convey emotional information. First, I create a list of the unique non-word characters present in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c24573",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = reviews[\"text\"].drop_duplicates()\n",
    "non_word = unique_tokens.str.extractall(r\"(\\W)\").drop_duplicates()\n",
    "non_word = non_word.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571ea57",
   "metadata": {},
   "source": [
    "Next, I reformat the definitions from the emoji dictionary that I obtained from the `emot` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a68511",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {}\n",
    "for meaning, symbol in EMOJI_UNICODE.items():\n",
    "    emojis[symbol] = meaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a6804",
   "metadata": {},
   "source": [
    "I also create a regex pattern that separates all the characters that I want to drop by a pipe, `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51976328/best-way-to-remove-xad-in-python\n",
    "# https://stackoverflow.com/questions/31522361/python-getting-rid-of-u200b-from-a-string-using-regular-expressions\n",
    "# https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "drop = [char for char in non_word if char not in emojis]\n",
    "drop.remove(\"'\")\n",
    "drop.remove(\"-\")\n",
    "drop = \"|\".join(drop) # Create regex pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62892f82",
   "metadata": {},
   "source": [
    "Using the pattern created above, I replace the specified non-word characters with empty strings. I drop these empty strings along with tokens that are comprised of consecutive hyphens. These tokens are used by reviewers as horizontal lines to format their pieces. However, I have no use for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.replace(drop, \"\", regex = True)\n",
    "reviews = reviews[(reviews[\"text\"] != \"\") & ~(reviews[\"text\"].str.fullmatch(r\"(-+)\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf0568",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f7327",
   "metadata": {},
   "source": [
    "I drop the rows of the table that belonged to stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e89917",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords = list(stopwords.words(\"English\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[~reviews[\"text\"].isin(stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fefbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.to_csv(\"data/intermediates/cleaned_text.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/cleaned_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5f175",
   "metadata": {},
   "source": [
    "## Add target field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed659b31",
   "metadata": {},
   "source": [
    "Here I define what I mean by a \"tearjerker\" anime. An anime is considered a tearjerker if its reviews feature at least one word from the `sad_words` list in addition to at least one from `cry_words`. This dual criteria serves as an indicator of whether an anime prompted its audience to weap from sadness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_words = [\"sad\",\n",
    "             \"saddest\",\n",
    "             \"emotion\",\n",
    "             \"emotions\",\n",
    "             \"emotional\",\n",
    "             \"emotionally\",\n",
    "             \"depressed\",\n",
    "             \"depressing\",\n",
    "             \"depressingly\",\n",
    "             \"tragic\",\n",
    "             \"tragedy\",\n",
    "             \"sentimental\"]\n",
    "\n",
    "cry_words = [\"cry\", \n",
    "             \"cried\", \n",
    "             \"crying\", \n",
    "             \"sob\", \n",
    "             \"sobbed\", \n",
    "             \"sobbing\", \n",
    "             \"bawl\", \n",
    "             \"bawled\", \n",
    "             \"bawling\", \n",
    "             \"tear\", \n",
    "             \"tears\", \n",
    "             \"teared\", \n",
    "             \"tearing\", # as in \"tearing up\"\n",
    "             \"sadorcrying\",\n",
    "             \"tearsofhappiness\",\n",
    "             \"sadofcrying\",\n",
    "             \"üò≠\"]\n",
    "\n",
    "reviews[\"sad\"] = reviews[\"text\"].isin(sad_words)\n",
    "reviews[\"cry\"] = reviews[\"text\"].isin(cry_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98309c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_vote = reviews.groupby([\"anime_uid\", \"uid\"]).agg(\n",
    "    sad = pd.NamedAgg(column = \"sad\", aggfunc = \"sum\"),\n",
    "    cry = pd.NamedAgg(column = \"cry\", aggfunc = \"sum\")\n",
    ")\n",
    "cry_vote[\"cry\"] = (cry_vote[\"sad\"] > 0) & (cry_vote[\"cry\"] > 0)\n",
    "cry_vote = cry_vote[\"cry\"].reset_index()\n",
    "cry_vote = cry_vote.groupby(\"anime_uid\").agg(\n",
    "    cry = pd.NamedAgg(column = \"cry\", aggfunc = \"mean\"),\n",
    "    reviews = pd.NamedAgg(column = \"uid\", aggfunc = \"nunique\")\n",
    ")\n",
    "cry_vote = cry_vote.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cry_vote = reviews.groupby(\"anime_uid\").agg(\n",
    "#     cry = pd.NamedAgg(column = \"cry\", aggfunc = \"sum\"),\n",
    "#     n = pd.NamedAgg(column = \"n\", aggfunc = \"mean\")\n",
    "# )\n",
    "# cry_vote = cry_vote.reset_index()\n",
    "# cry_vote[\"cry\"] = cry_vote[\"cry\"] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = cry_vote.merge(anime, how = \"left\", left_on = \"anime_uid\", right_on = \"uid\").drop(columns = \"anime_uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.to_csv(\"data/intermediates/anime.csv\", index = False)\n",
    "anime = pd.read_csv(\"data/intermediates/anime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ea8d0",
   "metadata": {},
   "source": [
    "# Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(anime, test_size = 0.2, random_state = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddad425",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv(\"data/intermediates/train.csv\", index = False)\n",
    "train_set = pd.read_csv(\"data/intermediates/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed377fdc",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d835cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb071f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"cry\"] = train_set[\"cry\"] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Proportion of tearjerker anime: {(train_set['cry']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f643b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 4))\n",
    "\n",
    "genres = train_set[[\"cry\", \"genre\"]].copy()\n",
    "genres[\"genre\"] = genres[\"genre\"].str.replace(r\"[\\[\\]']\", \"\", regex = True).str.split(\", \")\n",
    "genres = genres.explode(\"genre\")\n",
    "cry = genres[genres[\"cry\"]].groupby(\"genre\")[\"genre\"].count()\n",
    "cry = cry.rename(\"count\").reset_index().sort_values(by = \"count\").tail()\n",
    "no_cry = genres[~genres[\"cry\"]].groupby(\"genre\")[\"genre\"].count()\n",
    "no_cry = no_cry.rename(\"count\").reset_index().sort_values(by = \"count\").tail()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(cry[\"genre\"], cry[\"count\"])\n",
    "plt.title(\"Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(no_cry[\"genre\"], no_cry[\"count\"])\n",
    "plt.title(\"Non-Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/genres.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf69497",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 4))\n",
    "\n",
    "types = train_set[[\"cry\", \"type\"]]\n",
    "cry = types[types[\"cry\"]][[\"type\"]].groupby(\"type\")[\"type\"].count().sort_values()\n",
    "no_cry = types[~types[\"cry\"]][[\"type\"]].groupby(\"type\")[\"type\"].count().sort_values()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(cry.index, cry.values)\n",
    "plt.title(\"Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(no_cry.index, no_cry.values)\n",
    "plt.title(\"Non-Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/types.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "plot_num = 1\n",
    "for var in [\"score\", \"members\", \"episodes\", \"reviews\"]:\n",
    "    plt.subplot(2, 2, plot_num)\n",
    "    cry = train_set[train_set[\"cry\"]][var].dropna()\n",
    "    no_cry = train_set[~train_set[\"cry\"]][var].dropna()\n",
    "    plt.boxplot([cry, no_cry], vert = False, widths = 0.6,\n",
    "                showfliers = False, labels = [\"Tearjerker\", \"Non-Tearjerker\"])\n",
    "    plt.xlabel(var.capitalize())\n",
    "    plot_num += 1\n",
    "\n",
    "# https://www.geeksforgeeks.org/how-to-set-the-spacing-between-subplots-in-matplotlib-in-python/\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/numeric.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30f777",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime = anime[anime[\"title\"] != \"True Tears\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set[[\"cry\", \"score\", \"members\", \"synopsis\", \"genre\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113e4c3",
   "metadata": {},
   "source": [
    "## Drop anime with missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6cca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[\"cry\"]\n",
    "X_train = train_set[[\"score\", \"members\", \"synopsis\", \"genre\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee976b",
   "metadata": {},
   "source": [
    "## Clean `genre` and `studio` for `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca55720",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"genre\"] = X_train[\"genre\"].str.replace(\"[\\[\\]']\", \"\", regex = True).str.split(\", \")\n",
    "X_train[\"genre\"] = X_train[\"genre\"].apply(lambda genres: [genre.replace(\" \", \"\") for genre in genres])\n",
    "X_train[\"genre\"] = X_train[\"genre\"].apply(lambda genres: \" \".join(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime[\"studio\"] = anime[\"studio\"].str.split(\", \")\n",
    "# anime[\"studio\"] = anime[\"studio\"].apply(lambda studios: [studio.replace(\" \", \"\") for studio in studios])\n",
    "# anime[\"studio\"] = anime[\"studio\"].apply(lambda studios: \" \".join(studios))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de3435",
   "metadata": {},
   "source": [
    "## Clean `synopsis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607431c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = re.compile(r\"(?<=\\n)[^\\n]*$\")\n",
    "has_filler = X_train[\"synopsis\"].str.extractall(\"(.)$\").iloc[:, 0].str.match(\"[\\]\\)]\").reset_index(drop = True)\n",
    "X_train[\"synopsis\"][has_filler] = X_train[\"synopsis\"][has_filler].str.replace(filler, \"\").str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282cce2f",
   "metadata": {},
   "source": [
    "# Data transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b976f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    (\"Normalize score\", StandardScaler(), [\"score\"])\n",
    "    , (\"Normalize members\", StandardScaler(), [\"members\"])\n",
    "#     , (\"Normalize episodes\", StandardScaler(), [\"episodes\"])\n",
    "    # https://stackoverflow.com/questions/65242617/sklearn-pipeline-with-countvectorizer-and-category-on-a-pandas-dataframe\n",
    "    # https://stackoverflow.com/questions/58772181/columntransformer-fails-with-countvectorizer-in-a-pipeline\n",
    "#     , (\"Extract features from synopsis\", CountVectorizer(stop_words = stopwords), \"synopsis\")\n",
    "    , (\"Extract features from genre\", CountVectorizer(stop_words = stopwords), \"genre\")\n",
    "#     , (\"Extract features from studio\", CountVectorizer(stop_words = stopwords), \"studio\")\n",
    "#     , (\"One-hot encode type\", OneHotEncoder(), [\"type\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn\n",
    "X_train = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ec865",
   "metadata": {},
   "source": [
    "# Fit logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# model.fit(X_train, y_train)\n",
    "cross_val_score(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf2d7a",
   "metadata": {},
   "source": [
    "# Evaluate logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c601e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification\n",
    "# pred = model.predict_proba(X_test)[:, 1] > 0.48\n",
    "pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc815db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09958d8",
   "metadata": {},
   "source": [
    "# Fit decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0afb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# decision_tree.fit(X_train, y_train)\n",
    "cross_val_score(decision_tree, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d02049",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1801e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decision_tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4098bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b699dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa2429",
   "metadata": {},
   "source": [
    "# Fit random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfa0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9239d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# random_forest.fit(X_train, y_train)\n",
    "cross_val_score(random_forest, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a713f7a",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8370177",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = random_forest.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537069eb",
   "metadata": {},
   "source": [
    "# Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708dc1b",
   "metadata": {},
   "source": [
    "I used Aur√©lion G√©ron's *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* as a reference text during this project. In it, G√©ron emphasizes the importance of creating a test set prior to any modeling so that it can be used to honestly assess how well your final model generalizes. I initially failed to adhere to this rule and so had to generate a brand new test set by randomly resampling 25% of the data set after already fitting my models on a previously sampled 25%. (Although I generated a new test set, overfitting might still pose an issue since I had been modifying my target vector to improve performance on the previous test set, and some of the data from the previous test set is still present within the new test set.) Indeed, I was so euthusiastic about modeling that I didn't give every step of the machine learning process its due, specifically EDA. After starting over with a new test set, I decided to spend additional time on EDA *before* proceeding to modeling. Having learned these lessons, I know that I'll allot the appropriate amount of time to EDA and be more concious of data leakage in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f400aea",
   "metadata": {},
   "source": [
    "# Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ccb67",
   "metadata": {},
   "source": [
    "These projects were especially helpful in helping me create my own.\n",
    "\n",
    "- [Finding Anime Genre Based on Synopsis(Logistic Reg](https://www.kaggle.com/code/shsagar/finding-anime-genre-based-on-synopsis-logistic-reg)\n",
    "- [A Beginner's Guide to Sentiment Analysis with Python](https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6)\n",
    "\n",
    "I also relied on this [regular expression cheat sheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
