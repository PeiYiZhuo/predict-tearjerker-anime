{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94c2286",
   "metadata": {},
   "source": [
    "# When Computers Cry: Predicting Tearjerker Anime Using Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0797fa3",
   "metadata": {},
   "source": [
    "One of the few pieces of media to have ever coaxed tears from me is Kyoto Animation's devastating 2008 anime series *Clannad: After Story*, a show that I enthusiatically recommend. And I do so not in spite of the fact that it crushed my soul but because of it. If anything, I consider an anime's ability to turn on the waterworks to be a marker of quality. Most of my favorite anime are considered tearjerkers, *A Place Further than the Universe*, *Your Name*, and *K-On* come to mind as notable examples. Even if I don't tear up while watching them, a decent attempt made by an anime's creators still gets enormous credit from me. Given my interest in anime, I decided to take on the task of predicting tearjerker anime as a starter project for me to clarify and advance my understanding of machine learning and Scikit-learn.\n",
    "\n",
    "Using data scraped off of the website MyAnimeList courtesy of , here's what I did . . .\n",
    "\n",
    "1. Determined for anime whether each is a tearjerker or not based on the frequency of key words, \"cry\", \"tears,\" etc., in their reviews.\n",
    "2. Used the synopsis of the anime from MyAnimeList along with to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bf65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "# https://pub.towardsai.net/emoticon-and-emoji-in-text-mining-7392c49f596a\n",
    "# https://medium.com/geekculture/text-preprocessing-how-to-handle-emoji-emoticon-641bbfa6e9e7\n",
    "from emot.emo_unicode import EMOTICONS_EMO, EMOJI_UNICODE\n",
    "# https://www.kaggle.com/code/shsagar/finding-anime-genre-based-on-synopsis-logistic-reg\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd399243",
   "metadata": {},
   "source": [
    "# Wrangle Anime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0a4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews?select=animes.csv\n",
    "animes = pd.read_csv(\"data/animes.csv\") \n",
    "# https://www.kaggle.com/datasets/azathoth42/myanimelist?select=AnimeList.csv\n",
    "AnimeList = pd.read_csv(\"data/AnimeList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa86bccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>title_english</th>\n",
       "      <th>title_synonyms</th>\n",
       "      <th>score</th>\n",
       "      <th>members</th>\n",
       "      <th>type</th>\n",
       "      <th>episodes</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genre</th>\n",
       "      <th>source</th>\n",
       "      <th>studio</th>\n",
       "      <th>producer</th>\n",
       "      <th>licensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28891</td>\n",
       "      <td>Haikyuu!! Second Season</td>\n",
       "      <td>Haikyu!! 2nd Season</td>\n",
       "      <td>Haikyuu!! Second Season</td>\n",
       "      <td>8.82</td>\n",
       "      <td>489888</td>\n",
       "      <td>TV</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Following their participation at the Inter-Hig...</td>\n",
       "      <td>['Comedy', 'Sports', 'Drama', 'School', 'Shoun...</td>\n",
       "      <td>Manga</td>\n",
       "      <td>Production I.G</td>\n",
       "      <td>TOHO animation, Shueisha</td>\n",
       "      <td>Sentai Filmworks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23273</td>\n",
       "      <td>Shigatsu wa Kimi no Uso</td>\n",
       "      <td>Your Lie in April</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.83</td>\n",
       "      <td>995473</td>\n",
       "      <td>TV</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Music accompanies the path of the human metron...</td>\n",
       "      <td>['Drama', 'Music', 'Romance', 'School', 'Shoun...</td>\n",
       "      <td>Manga</td>\n",
       "      <td>A-1 Pictures</td>\n",
       "      <td>Aniplex, Dentsu, Kodansha, Fuji TV, Kyoraku In...</td>\n",
       "      <td>Aniplex of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34599</td>\n",
       "      <td>Made in Abyss</td>\n",
       "      <td>Made in Abyss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.83</td>\n",
       "      <td>581663</td>\n",
       "      <td>TV</td>\n",
       "      <td>13.0</td>\n",
       "      <td>The Abyssâ€”a gaping chasm stretching down into ...</td>\n",
       "      <td>['Sci-Fi', 'Adventure', 'Mystery', 'Drama', 'F...</td>\n",
       "      <td>Web manga</td>\n",
       "      <td>Kinema Citrus</td>\n",
       "      <td>Media Factory, AT-X, Takeshobo, Sony Music Com...</td>\n",
       "      <td>Sentai Filmworks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5114</td>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>Hagane no Renkinjutsushi: Fullmetal Alchemist,...</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1615084</td>\n",
       "      <td>TV</td>\n",
       "      <td>64.0</td>\n",
       "      <td>\"In order for something to be obtained, someth...</td>\n",
       "      <td>['Action', 'Military', 'Adventure', 'Comedy', ...</td>\n",
       "      <td>Manga</td>\n",
       "      <td>Bones</td>\n",
       "      <td>Aniplex, Square Enix, Mainichi Broadcasting Sy...</td>\n",
       "      <td>Funimation, Aniplex of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31758</td>\n",
       "      <td>Kizumonogatari III: Reiketsu-hen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Koyomi Vamp, Kizumonogatari Part 3</td>\n",
       "      <td>8.83</td>\n",
       "      <td>214621</td>\n",
       "      <td>Movie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>After helping revive the legendary vampire Kis...</td>\n",
       "      <td>['Action', 'Mystery', 'Supernatural', 'Vampire']</td>\n",
       "      <td>Light novel</td>\n",
       "      <td>Shaft</td>\n",
       "      <td>Aniplex, Kodansha</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     uid                             title                     title_english  \\\n",
       "0  28891           Haikyuu!! Second Season               Haikyu!! 2nd Season   \n",
       "1  23273           Shigatsu wa Kimi no Uso                 Your Lie in April   \n",
       "2  34599                     Made in Abyss                     Made in Abyss   \n",
       "3   5114  Fullmetal Alchemist: Brotherhood  Fullmetal Alchemist: Brotherhood   \n",
       "4  31758  Kizumonogatari III: Reiketsu-hen                               NaN   \n",
       "\n",
       "                                      title_synonyms  score  members   type  \\\n",
       "0                            Haikyuu!! Second Season   8.82   489888     TV   \n",
       "1                                                NaN   8.83   995473     TV   \n",
       "2                                                NaN   8.83   581663     TV   \n",
       "3  Hagane no Renkinjutsushi: Fullmetal Alchemist,...   9.23  1615084     TV   \n",
       "4                 Koyomi Vamp, Kizumonogatari Part 3   8.83   214621  Movie   \n",
       "\n",
       "   episodes                                           synopsis  \\\n",
       "0      25.0  Following their participation at the Inter-Hig...   \n",
       "1      22.0  Music accompanies the path of the human metron...   \n",
       "2      13.0  The Abyssâ€”a gaping chasm stretching down into ...   \n",
       "3      64.0  \"In order for something to be obtained, someth...   \n",
       "4       1.0  After helping revive the legendary vampire Kis...   \n",
       "\n",
       "                                               genre       source  \\\n",
       "0  ['Comedy', 'Sports', 'Drama', 'School', 'Shoun...        Manga   \n",
       "1  ['Drama', 'Music', 'Romance', 'School', 'Shoun...        Manga   \n",
       "2  ['Sci-Fi', 'Adventure', 'Mystery', 'Drama', 'F...    Web manga   \n",
       "3  ['Action', 'Military', 'Adventure', 'Comedy', ...        Manga   \n",
       "4   ['Action', 'Mystery', 'Supernatural', 'Vampire']  Light novel   \n",
       "\n",
       "           studio                                           producer  \\\n",
       "0  Production I.G                           TOHO animation, Shueisha   \n",
       "1    A-1 Pictures  Aniplex, Dentsu, Kodansha, Fuji TV, Kyoraku In...   \n",
       "2   Kinema Citrus  Media Factory, AT-X, Takeshobo, Sony Music Com...   \n",
       "3           Bones  Aniplex, Square Enix, Mainichi Broadcasting Sy...   \n",
       "4           Shaft                                  Aniplex, Kodansha   \n",
       "\n",
       "                         licensor  \n",
       "0                Sentai Filmworks  \n",
       "1              Aniplex of America  \n",
       "2                Sentai Filmworks  \n",
       "3  Funimation, Aniplex of America  \n",
       "4                             NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnimeList = AnimeList[[\"anime_id\", \n",
    "                       \"title_english\", \n",
    "                       \"title_synonyms\", \n",
    "                       \"type\", \n",
    "                       \"source\", \n",
    "                       \"producer\", \n",
    "                       \"licensor\", \n",
    "                       \"studio\"]]\n",
    "animes = animes.merge(AnimeList, how = \"left\", left_on = \"uid\", right_on = \"anime_id\")\n",
    "anime = animes[[\"uid\", \n",
    "                \"title\", \n",
    "                \"title_english\", \n",
    "                \"title_synonyms\",\n",
    "                \"score\",\n",
    "                \"members\",\n",
    "                \"type\",\n",
    "                \"episodes\",\n",
    "                \"synopsis\", \n",
    "                \"genre\", \n",
    "                \"source\", \n",
    "                \"studio\",\n",
    "                \"producer\", \n",
    "                \"licensor\"]]\n",
    "anime = anime.drop_duplicates()\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d55bdc",
   "metadata": {},
   "source": [
    "# What is a tearjerker anime?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff2e23",
   "metadata": {},
   "source": [
    "In order to predict tearjerker anime, I must define it. To construct this target vector, I used a dataset of anime reviews. Thus, the determination of whether an anime is a tearjerker or not is based upon reviewers' reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.read_csv(\"data/reviews.csv\")\n",
    "# reviews = reviews.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3335c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# total_rows = len(reviews)\n",
    "# max_rows = 10000\n",
    "# num_files = math.ceil(total_rows/max_rows)\n",
    "\n",
    "# start = 0\n",
    "# end = 9999\n",
    "\n",
    "# for i in range(1, num_files + 1):\n",
    "#     i = str(i)\n",
    "#     print(\"Writing file #\" + i)\n",
    "#     reviews.iloc[start:(end + 1), :].to_csv(\"data/reviews/reviews\" + i + \".csv\", index = False)\n",
    "#     start += 10000\n",
    "#     end += 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8840c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "reviews = pd.read_csv(\"data/reviews/reviews1.csv\")\n",
    "for i in range(2, 15):\n",
    "    i = str(i)\n",
    "    print(f\"Concatenating data file #{i}\")\n",
    "    addition = pd.read_csv(f\"data/reviews/reviews{i}.csv\")\n",
    "    reviews = pd.concat([reviews, addition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant fields\n",
    "reviews = reviews[[\"uid\", \"anime_uid\", \"link\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counts = reviews.groupby(\"anime_uid\")[\"anime_uid\"].count() \n",
    "keep_anime = review_counts[review_counts >= 10].index.tolist()\n",
    "reviews = reviews[reviews[\"anime_uid\"].isin(keep_anime)]\n",
    "reviews = reviews.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = reviews.groupby(\"anime_uid\")[\"uid\"].nunique().rename(\"n\")\n",
    "# reviews = reviews.sort_values([\"anime_uid\", \"uid\"]).groupby(\"anime_uid\").head(1)\n",
    "# reviews = reviews.merge(n, how = \"left\", left_on = \"anime_uid\", right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481d3f7",
   "metadata": {},
   "source": [
    "## Remove front and back matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba92f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = re.compile(r\"^[\\s\\w]*Enjoyment[\\s\\d]*|\\s*Helpful\\s*$\") # Assumes reviews never start with a number\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(filler, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea346fe",
   "metadata": {},
   "source": [
    "## Replace emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e197e",
   "metadata": {},
   "source": [
    "The reviews feature heavy use of emoticons. These symbols allow users to succintly communicate an emotional reaction through pictograms comprised of punctuation, letters, numbers, etc. Because they convey information on emotion, I want to retain them to help me determine whether an anime is a tearjerker or not. However, since they include punctuation, which will be removed from the text later on in the process of constructing the target vector, I opted to replace them with verbal descriptions.\n",
    "\n",
    "I used a dictionary of emoticons from the `emot` library ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba493fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://introtopython.org/dictionaries.html#General-Syntax\n",
    "emoticons = {}\n",
    "for symbol, meaning in EMOTICONS_EMO.items():\n",
    "    emoticons[symbol] = \"\".join([word.capitalize() for word in meaning.replace(\",\", \"\").split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ea7",
   "metadata": {},
   "source": [
    "... and added a few missing emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons[\"(^â€”^)\"] = \"NormalLaugh\"\n",
    "emoticons[\"-_-â€œ\"] = \"Troubled\" \n",
    "emoticons[\":s)\"] = \"HappyFaceOrSmiley\" \n",
    "emoticons[\":S)\"] = \"HappyFaceOrSmiley\" \n",
    "# https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "emoticons[\">W<\"] = \"Troubled\"\n",
    "emoticons[\"-_-'\"] = \"Troubled\"\n",
    "# https://www.urbandictionary.com/define.php?term=%3E_%3E\n",
    "emoticons[\">_>\"] = \"RightSidewaysLook\"\n",
    "emoticons[\"<_<\"] = \"LeftSidewaysLook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5788f9a8",
   "metadata": {},
   "source": [
    "Since many long emoticons are simply short ones with additional characters, I sorted the emoticons in descending order according to their length so that, later on, the longer emoticons would be matched prior to shorter emoticons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "# https://www.w3schools.com/python/ref_func_sorted.asp\n",
    "emoticons = dict(sorted(emoticons.items(), key = lambda item: -len(item[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5895d",
   "metadata": {},
   "source": [
    "I then assembled a list of emoticons that feature in both the dictionary and the reviews. I managed to get 97 unique emoticons, but I'm sure I missed some T_T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pythonforbeginners.com/basics/list-comprehensions-in-python\n",
    "# https://www.geeksforgeeks.org/python-accessing-key-value-in-dictionary/\n",
    "# https://stackoverflow.com/questions/4202538/escape-special-characters-in-a-python-string\n",
    "pattern = \"\\s(\" + \"|\".join([re.escape(emoticon) for emoticon in emoticons]) + \")\\W?\"\n",
    "used_emoticons = reviews[\"text\"].str.extractall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f32092",
   "metadata": {},
   "source": [
    "Here, I'm converting the dataframe of used emoticons into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/python-convert-numpy-array-to-list\n",
    "used_emoticons = used_emoticons.dropna().drop_duplicates().iloc[:, 0].tolist()\n",
    "# https://stackoverflow.com/questions/5352546/extract-subset-of-key-value-pairs-from-dictionary\n",
    "used_emoticons = {symbol: emoticons[symbol] for symbol in used_emoticons}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f858c4b",
   "metadata": {},
   "source": [
    "Again, I'm sorting the emoticons according to their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_emoticons = dict(sorted(used_emoticons.items(), key = lambda item: -len(item[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a26df",
   "metadata": {},
   "source": [
    "Now, I'm looping through each emoticon and replacing it with its respective value in the dictionary if it's preceded by a white space and followed by a non-word character. These conditions are meant to reduce the likelihood of false-positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef79d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, meaning in used_emoticons.items():\n",
    "    print(f\"Replacing {symbol} with {meaning}\")\n",
    "    reviews[\"text\"] = reviews[\"text\"].str.replace(\"(?<=\\s)\" + re.escape(symbol) + \"(?=\\W?)\", meaning, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv(\"data/intermediates/replace_emoticons.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/replace_emoticons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e69c7",
   "metadata": {},
   "source": [
    "## Drop titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd05bff",
   "metadata": {},
   "source": [
    "I use the appearance of key words such as \"cry\" and \"tears\" in user reviews to determine whether an anime is a tearjerker or not. Some anime include these key words in their titles, meaning users really can't help but mention them in their reviews. Thus, to ensure that I'll only be counting authentic appearances of these terms, I exclude the title of the anime being discussed from the text of each review.\n",
    "\n",
    "*True Tears* is a peculiar case because one of its characters' ability to cry is a central plot point. Thus, reviewers who are summarizing its story mention crying quite a lot even after scrubbing its reviews of any mention of the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db82f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.merge(\n",
    "    anime[[\"uid\", \"title\", \"title_english\", \"title_synonyms\"]], \n",
    "    how = \"left\", left_on = \"anime_uid\", right_on = \"uid\"\n",
    ")\n",
    "reviews = reviews.drop(columns = \"uid_y\").rename(columns = {\"uid_x\": \"uid\"})\n",
    "reviews[\"text\"] = reviews[\"text\"].str.lower()\n",
    "reviews[\"title\"] = reviews[\"title\"].str.lower()\n",
    "reviews[\"title_english\"] = reviews[\"title_english\"].str.lower()\n",
    "reviews[\"title_synonyms\"] = reviews[\"title_synonyms\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in [\"title\", \"title_english\", \"title_synonyms\"]:\n",
    "    for i in range(len(reviews)):\n",
    "        print(f\"Processing row #{str(i)} for {field}\")\n",
    "        title = reviews[field].iloc[i]\n",
    "        if not pd.isna(title):\n",
    "            unigrams = reviews[field].iloc[[i]].str.split().iloc[0]\n",
    "            # http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "            bigrams = [\" \".join(unigram) for unigram in list(zip(unigrams, unigrams[1:]))]\n",
    "            pattern = \"|\".join([re.escape(title)] + [re.escape(bigram) for bigram in bigrams])\n",
    "            reviews[\"text\"].iloc[[i]] = reviews[\"text\"].iloc[[i]].str.replace(pattern, \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6458131",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop(columns = [\"title\", \"title_english\", \"title_synonyms\"])\n",
    "reviews.to_csv(\"data/intermediates/drop_titles.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/drop_titles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b58543",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17513cb",
   "metadata": {},
   "source": [
    "I turn the table of reviews where each row is one review into a table of tokens where each row is one token from a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.strip()\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"\\\\\", \" \", regex = True)\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"/\", \" \")                                      \n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"â€˜\", \"'\").str.replace(\"â€™\", \"'\")\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"â€œ\", '\"').str.replace(\"â€\", '\"')\n",
    "# \"â€“\" is used as punctuation while \"-\" is used to create phrases\n",
    "pattern = \"[\" + string.punctuation.replace(\"'\", \"\").replace(\"-\", \"\") + \"â€“\" + \"â€¦\" + \"]\" \n",
    "pattern = pattern + r\"|(?<=\\s)'(?=\\w)|(?<=\\w)'(?=\\s)\"\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(pattern, \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.split()\n",
    "reviews = reviews.explode(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf784fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv(\"data/intermediates/tokenize_text.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/tokenize_text.csv\", na_filter = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555994d",
   "metadata": {},
   "source": [
    "## Replace non-word characters except for emojis and select punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca51a32",
   "metadata": {},
   "source": [
    "To further cleanse the tokens, I want to drop all non-word characters that aren't emojis. For the same reason as emoticons, I want to use emojis to create my target vector because they convey emotional information. First, I create a list of the unique non-word characters present in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c24573",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = reviews[\"text\"].drop_duplicates()\n",
    "non_word = unique_tokens.str.extractall(r\"(\\W)\").drop_duplicates()\n",
    "non_word = non_word.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571ea57",
   "metadata": {},
   "source": [
    "Next, I reformat the definitions from the emoji dictionary that I obtained from the `emot` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a68511",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {}\n",
    "for meaning, symbol in EMOJI_UNICODE.items():\n",
    "    emojis[symbol] = meaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a6804",
   "metadata": {},
   "source": [
    "I also create a regex pattern that separates all the characters that I want to drop by a pipe, `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51976328/best-way-to-remove-xad-in-python\n",
    "# https://stackoverflow.com/questions/31522361/python-getting-rid-of-u200b-from-a-string-using-regular-expressions\n",
    "# https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "drop = [char for char in non_word if char not in emojis]\n",
    "drop.remove(\"'\")\n",
    "drop.remove(\"-\")\n",
    "drop = \"|\".join(drop) # Create regex pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62892f82",
   "metadata": {},
   "source": [
    "Using the pattern created above, I replace the specified non-word characters with empty strings. I drop these empty strings along with tokens that are comprised of consecutive hyphens. These tokens are used by reviewers as horizontal lines to format their pieces. However, I have no use for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.replace(drop, \"\", regex = True)\n",
    "reviews = reviews[(reviews[\"text\"] != \"\") & ~(reviews[\"text\"].str.fullmatch(r\"(-+)\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf0568",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705290ac",
   "metadata": {},
   "source": [
    "I drop the rows of the table that belonged to stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e89917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/peiyizhuo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords = list(stopwords.words(\"English\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[~reviews[\"text\"].isin(stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8fefbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.to_csv(\"data/intermediates/cleaned_text.csv\", index = False)\n",
    "reviews = pd.read_csv(\"data/intermediates/cleaned_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5f175",
   "metadata": {},
   "source": [
    "## Add target field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ecf4bb",
   "metadata": {},
   "source": [
    "Here I define what I mean by a \"tearjerker\" anime. An anime is considered a tearjerker if its reviews feature at least one word from the `sad_words` list in addition to at least one from `cry_words`. This dual criteria serves as an indicator of whether an anime prompted its audience to weap from sadness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a058db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_words = [\"sad\",\n",
    "             \"saddest\",\n",
    "             \"emotion\",\n",
    "             \"emotions\",\n",
    "             \"emotional\",\n",
    "             \"emotionally\",\n",
    "             \"depressed\",\n",
    "             \"depressing\",\n",
    "             \"depressingly\",\n",
    "             \"tragic\",\n",
    "             \"tragedy\",\n",
    "             \"sentimental\"]\n",
    "\n",
    "cry_words = [\"cry\", \n",
    "             \"cried\", \n",
    "             \"crying\", \n",
    "             \"sob\", \n",
    "             \"sobbed\", \n",
    "             \"sobbing\", \n",
    "             \"bawl\", \n",
    "             \"bawled\", \n",
    "             \"bawling\", \n",
    "             \"tear\", \n",
    "             \"tears\", \n",
    "             \"teared\", \n",
    "             \"tearing\", # as in \"tearing up\"\n",
    "             \"sadorcrying\",\n",
    "             \"tearsofhappiness\",\n",
    "             \"sadofcrying\",\n",
    "             \"ðŸ˜­\"]\n",
    "\n",
    "reviews[\"sad\"] = reviews[\"text\"].isin(sad_words)\n",
    "reviews[\"cry\"] = reviews[\"text\"].isin(cry_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fa09ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_vote = reviews.groupby([\"anime_uid\", \"uid\"]).agg(\n",
    "    sad = pd.NamedAgg(column = \"sad\", aggfunc = \"sum\"),\n",
    "    cry = pd.NamedAgg(column = \"cry\", aggfunc = \"sum\")\n",
    ")\n",
    "cry_vote[\"cry\"] = (cry_vote[\"sad\"] > 0) & (cry_vote[\"cry\"] > 0)\n",
    "cry_vote = cry_vote[\"cry\"].reset_index()\n",
    "cry_vote = cry_vote.groupby(\"anime_uid\").agg(\n",
    "    cry = pd.NamedAgg(column = \"cry\", aggfunc = \"mean\"),\n",
    "    n = pd.NamedAgg(column = \"uid\", aggfunc = \"nunique\")\n",
    ")\n",
    "cry_vote = cry_vote.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cry_vote = reviews.groupby(\"anime_uid\").agg(\n",
    "#     cry = pd.NamedAgg(column = \"cry\", aggfunc = \"sum\"),\n",
    "#     n = pd.NamedAgg(column = \"n\", aggfunc = \"mean\")\n",
    "# )\n",
    "# cry_vote = cry_vote.reset_index()\n",
    "# cry_vote[\"cry\"] = cry_vote[\"cry\"] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76ce3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = cry_vote.merge(anime, how = \"left\", left_on = \"anime_uid\", right_on = \"uid\").drop(columns = \"anime_uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eee456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime.to_csv(\"data/intermediates/anime.csv\", index = False)\n",
    "anime = pd.read_csv(\"data/intermediates/anime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35f6cc",
   "metadata": {},
   "source": [
    "# Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a066f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(anime, test_size = 0.2, random_state = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d0ea811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set.to_csv(\"data/intermediates/train.csv\", index = False)\n",
    "train_set = pd.read_csv(\"data/intermediates/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed377fdc",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10968db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cry</th>\n",
       "      <th>n</th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>title_english</th>\n",
       "      <th>title_synonyms</th>\n",
       "      <th>score</th>\n",
       "      <th>members</th>\n",
       "      <th>type</th>\n",
       "      <th>episodes</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genre</th>\n",
       "      <th>source</th>\n",
       "      <th>studio</th>\n",
       "      <th>producer</th>\n",
       "      <th>licensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>32271</td>\n",
       "      <td>Dies Irae</td>\n",
       "      <td>Dies irae</td>\n",
       "      <td>Day of Wrath</td>\n",
       "      <td>5.46</td>\n",
       "      <td>97796</td>\n",
       "      <td>TV</td>\n",
       "      <td>11.0</td>\n",
       "      <td>On May 1, 1945 in Berlin, as the Red Army rais...</td>\n",
       "      <td>['Action', 'Military', 'Super Power', 'Magic']</td>\n",
       "      <td>Visual novel</td>\n",
       "      <td>A.C.G.T.</td>\n",
       "      <td>Genco, DMM pictures, Greenwood, My Theater D.D...</td>\n",
       "      <td>Funimation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.088889</td>\n",
       "      <td>45</td>\n",
       "      <td>856</td>\n",
       "      <td>Utawarerumono</td>\n",
       "      <td>Utawarerumono</td>\n",
       "      <td>The One Being Sung</td>\n",
       "      <td>7.71</td>\n",
       "      <td>126612</td>\n",
       "      <td>TV</td>\n",
       "      <td>26.0</td>\n",
       "      <td>An injured man is found in the woods by a girl...</td>\n",
       "      <td>['Action', 'Drama', 'Fantasy', 'Sci-Fi']</td>\n",
       "      <td>Visual novel</td>\n",
       "      <td>OLM</td>\n",
       "      <td>Lantis, Half H.P Studio, AQUAPLUS</td>\n",
       "      <td>ADV Films, Funimation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>17821</td>\n",
       "      <td>Stella Jogakuin Koutou-ka CÂ³-bu</td>\n",
       "      <td>Stella Women&amp;#039;s Academy, High School Divis...</td>\n",
       "      <td>Stella Jogakuin Koutouka C3-bu, Stella Jogakui...</td>\n",
       "      <td>6.57</td>\n",
       "      <td>46285</td>\n",
       "      <td>TV</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Yura Yamato has just arrived at the high schoo...</td>\n",
       "      <td>['Military', 'School', 'Sports']</td>\n",
       "      <td>Manga</td>\n",
       "      <td>Gainax</td>\n",
       "      <td>Pony Canyon, TBS, RAY</td>\n",
       "      <td>Sentai Filmworks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>2969</td>\n",
       "      <td>Appleseed Saga Ex Machina</td>\n",
       "      <td>Appleseed: Ex Machina</td>\n",
       "      <td>Appleseed 2, Appleseed 2007</td>\n",
       "      <td>7.41</td>\n",
       "      <td>33606</td>\n",
       "      <td>Movie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Deunan, a young female warrior, and Briareos, ...</td>\n",
       "      <td>['Action', 'Mecha', 'Military', 'Sci-Fi']</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Digital Frontier</td>\n",
       "      <td>Sega</td>\n",
       "      <td>ADV Films, Warner Bros.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>257</td>\n",
       "      <td>Ikkitousen</td>\n",
       "      <td>Ikki Tousen</td>\n",
       "      <td>Ikki-Tosen: Legendary Fighter, Battle Vixens</td>\n",
       "      <td>6.53</td>\n",
       "      <td>118837</td>\n",
       "      <td>TV</td>\n",
       "      <td>13.0</td>\n",
       "      <td>In  Ikkitousen , the Kanto region of Japan is ...</td>\n",
       "      <td>['Ecchi', 'Super Power', 'Martial Arts', 'Scho...</td>\n",
       "      <td>Manga</td>\n",
       "      <td>J.C.Staff</td>\n",
       "      <td>Genco, Cosmic Ray, Eye Move, Bushiroad</td>\n",
       "      <td>Funimation, Geneon Entertainment USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cry   n    uid                            title  \\\n",
       "0  0.000000  20  32271                        Dies Irae   \n",
       "1  0.088889  45    856                    Utawarerumono   \n",
       "2  0.000000  20  17821  Stella Jogakuin Koutou-ka CÂ³-bu   \n",
       "3  0.000000  14   2969        Appleseed Saga Ex Machina   \n",
       "4  0.000000  30    257                       Ikkitousen   \n",
       "\n",
       "                                       title_english  \\\n",
       "0                                          Dies irae   \n",
       "1                                      Utawarerumono   \n",
       "2  Stella Women&#039;s Academy, High School Divis...   \n",
       "3                              Appleseed: Ex Machina   \n",
       "4                                        Ikki Tousen   \n",
       "\n",
       "                                      title_synonyms  score  members   type  \\\n",
       "0                                       Day of Wrath   5.46    97796     TV   \n",
       "1                                 The One Being Sung   7.71   126612     TV   \n",
       "2  Stella Jogakuin Koutouka C3-bu, Stella Jogakui...   6.57    46285     TV   \n",
       "3                        Appleseed 2, Appleseed 2007   7.41    33606  Movie   \n",
       "4       Ikki-Tosen: Legendary Fighter, Battle Vixens   6.53   118837     TV   \n",
       "\n",
       "   episodes                                           synopsis  \\\n",
       "0      11.0  On May 1, 1945 in Berlin, as the Red Army rais...   \n",
       "1      26.0  An injured man is found in the woods by a girl...   \n",
       "2      13.0  Yura Yamato has just arrived at the high schoo...   \n",
       "3       1.0  Deunan, a young female warrior, and Briareos, ...   \n",
       "4      13.0  In  Ikkitousen , the Kanto region of Japan is ...   \n",
       "\n",
       "                                               genre        source  \\\n",
       "0     ['Action', 'Military', 'Super Power', 'Magic']  Visual novel   \n",
       "1           ['Action', 'Drama', 'Fantasy', 'Sci-Fi']  Visual novel   \n",
       "2                   ['Military', 'School', 'Sports']         Manga   \n",
       "3          ['Action', 'Mecha', 'Military', 'Sci-Fi']       Unknown   \n",
       "4  ['Ecchi', 'Super Power', 'Martial Arts', 'Scho...         Manga   \n",
       "\n",
       "             studio                                           producer  \\\n",
       "0          A.C.G.T.  Genco, DMM pictures, Greenwood, My Theater D.D...   \n",
       "1               OLM                  Lantis, Half H.P Studio, AQUAPLUS   \n",
       "2            Gainax                              Pony Canyon, TBS, RAY   \n",
       "3  Digital Frontier                                               Sega   \n",
       "4         J.C.Staff             Genco, Cosmic Ray, Eye Move, Bushiroad   \n",
       "\n",
       "                               licensor  \n",
       "0                            Funimation  \n",
       "1                 ADV Films, Funimation  \n",
       "2                      Sentai Filmworks  \n",
       "3               ADV Films, Warner Bros.  \n",
       "4  Funimation, Geneon Entertainment USA  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "89d8bcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1836 entries, 0 to 1835\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   cry             1836 non-null   float64\n",
      " 1   n               1836 non-null   int64  \n",
      " 2   uid             1836 non-null   int64  \n",
      " 3   title           1836 non-null   object \n",
      " 4   title_english   1340 non-null   object \n",
      " 5   title_synonyms  1215 non-null   object \n",
      " 6   score           1836 non-null   float64\n",
      " 7   members         1836 non-null   int64  \n",
      " 8   type            1762 non-null   object \n",
      " 9   episodes        1833 non-null   float64\n",
      " 10  synopsis        1835 non-null   object \n",
      " 11  genre           1836 non-null   object \n",
      " 12  source          1762 non-null   object \n",
      " 13  studio          1694 non-null   object \n",
      " 14  producer        1513 non-null   object \n",
      " 15  licensor        1254 non-null   object \n",
      "dtypes: float64(3), int64(3), object(10)\n",
      "memory usage: 229.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6fd3126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"cry\"] = train_set[\"cry\"] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68822355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of tearjerker anime: 0.5065359477124183\n"
     ]
    }
   ],
   "source": [
    "print(f\"Proportion of tearjerker anime: {(train_set['cry']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4b5f8a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7f9b7ef0d5e0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f9b7ef0d940>,\n",
       "  <matplotlib.lines.Line2D at 0x7f9b7ef19a60>,\n",
       "  <matplotlib.lines.Line2D at 0x7f9b7ef19dc0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7f9b7ef0dca0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f9b7ef19040>,\n",
       "  <matplotlib.lines.Line2D at 0x7f9b7ef24160>,\n",
       "  <matplotlib.lines.Line2D at 0x7f9b7ef244c0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7f9b7ef0d280>,\n",
       "  <matplotlib.lines.Line2D at 0x7f9b7ef19700>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7f9b7ef193a0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f9b7ef24820>],\n",
       " 'fliers': [],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "cry_score = train_set[train_set[\"cry\"]][\"score\"].dropna()\n",
    "no_cry_score = train_set[~train_set[\"cry\"]][\"score\"].dropna()\n",
    "plt.boxplot([cry_score, no_cry_score], vert = False, showfliers = False)\n",
    "plt.title(\"score\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "cry_members = train_set[train_set[\"cry\"]][\"members\"]\n",
    "no_cry_members = train_set[~train_set[\"cry\"]][\"members\"]\n",
    "plt.boxplot([cry_members, no_cry_members], vert = False, showfliers = False)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "cry_episodes = train_set[train_set[\"cry\"]][\"episodes\"].dropna()\n",
    "no_cry_episodes = train_set[~train_set[\"cry\"]][\"episodes\"].dropna()\n",
    "plt.boxplot([cry_episodes, no_cry_episodes], vert = False, showfliers = False)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "cry_reviewers = train_set[train_set[\"cry\"]][\"n\"]\n",
    "no_cry_reviewers = train_set[~train_set[\"cry\"]][\"n\"]\n",
    "plt.boxplot([cry_reviewers, no_cry_reviewers], vert = False, showfliers = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30f777",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime = anime[anime[\"title\"] != \"True Tears\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d49ebfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set[[\"cry\", \"score\", \"members\", \"synopsis\", \"genre\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8d6de",
   "metadata": {},
   "source": [
    "## Drop anime with missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a33e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1836 entries, 0 to 1835\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   cry       1836 non-null   bool   \n",
      " 1   score     1836 non-null   float64\n",
      " 2   members   1836 non-null   int64  \n",
      " 3   synopsis  1835 non-null   object \n",
      " 4   genre     1836 non-null   object \n",
      "dtypes: bool(1), float64(1), int64(1), object(2)\n",
      "memory usage: 59.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec40ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce2c045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[\"cry\"]\n",
    "X_train = train_set[[\"score\", \"members\", \"synopsis\", \"genre\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee976b",
   "metadata": {},
   "source": [
    "## Clean `genre` and `studio` for `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ca55720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-66-eab3bee462ac>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"genre\"] = X_train[\"genre\"].str.replace(\"[\\[\\]']\", \"\", regex = True).str.split(\", \")\n",
      "<ipython-input-66-eab3bee462ac>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"genre\"] = X_train[\"genre\"].apply(lambda genres: [genre.replace(\" \", \"\") for genre in genres])\n",
      "<ipython-input-66-eab3bee462ac>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"genre\"] = X_train[\"genre\"].apply(lambda genres: \" \".join(genres))\n"
     ]
    }
   ],
   "source": [
    "X_train[\"genre\"] = X_train[\"genre\"].str.replace(\"[\\[\\]']\", \"\", regex = True).str.split(\", \")\n",
    "X_train[\"genre\"] = X_train[\"genre\"].apply(lambda genres: [genre.replace(\" \", \"\") for genre in genres])\n",
    "X_train[\"genre\"] = X_train[\"genre\"].apply(lambda genres: \" \".join(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28ccf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime[\"studio\"] = anime[\"studio\"].str.split(\", \")\n",
    "# anime[\"studio\"] = anime[\"studio\"].apply(lambda studios: [studio.replace(\" \", \"\") for studio in studios])\n",
    "# anime[\"studio\"] = anime[\"studio\"].apply(lambda studios: \" \".join(studios))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de3435",
   "metadata": {},
   "source": [
    "## Clean `synopsis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "607431c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-d98ab17dd08c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"synopsis\"][has_filler] = X_train[\"synopsis\"][has_filler].str.replace(filler, \"\").str.strip()\n",
      "/Users/peiyizhuo/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:992: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._where(~key, value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "filler = re.compile(r\"(?<=\\n)[^\\n]*$\")\n",
    "has_filler = X_train[\"synopsis\"].str.extractall(\"(.)$\").iloc[:, 0].str.match(\"[\\]\\)]\").reset_index(drop = True)\n",
    "X_train[\"synopsis\"][has_filler] = X_train[\"synopsis\"][has_filler].str.replace(filler, \"\").str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282cce2f",
   "metadata": {},
   "source": [
    "# Data transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b976f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    (\"Normalize score\", StandardScaler(), [\"score\"])\n",
    "    , (\"Normalize members\", StandardScaler(), [\"members\"])\n",
    "#     , (\"Normalize episodes\", StandardScaler(), [\"episodes\"])\n",
    "    # https://stackoverflow.com/questions/65242617/sklearn-pipeline-with-countvectorizer-and-category-on-a-pandas-dataframe\n",
    "    # https://stackoverflow.com/questions/58772181/columntransformer-fails-with-countvectorizer-in-a-pipeline\n",
    "#     , (\"Extract features from synopsis\", CountVectorizer(stop_words = stopwords), \"synopsis\")\n",
    "    , (\"Extract features from genre\", CountVectorizer(stop_words = stopwords), \"genre\")\n",
    "#     , (\"Extract features from studio\", CountVectorizer(stop_words = stopwords), \"studio\")\n",
    "#     , (\"One-hot encode type\", OneHotEncoder(), [\"type\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9000e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn\n",
    "X_train = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ec865",
   "metadata": {},
   "source": [
    "# Fit logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e8bd267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31ae02ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67574932, 0.66757493, 0.69209809, 0.71662125, 0.71117166])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# model.fit(X_train, y_train)\n",
    "cross_val_score(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf2d7a",
   "metadata": {},
   "source": [
    "# Evaluate logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c601e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31417487/sklearn-logisticregression-and-changing-the-default-threshold-for-classification\n",
    "# pred = model.predict_proba(X_test)[:, 1] > 0.48\n",
    "pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc815db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d90040",
   "metadata": {},
   "source": [
    "# Fit decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ea8887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5f74dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60217984, 0.63215259, 0.62942779, 0.64305177, 0.64577657])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# decision_tree.fit(X_train, y_train)\n",
    "cross_val_score(decision_tree, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d6191",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decision_tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e3596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa2429",
   "metadata": {},
   "source": [
    "# Fit random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4cfa0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed9239d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66485014, 0.67302452, 0.67847411, 0.68392371, 0.70027248])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# random_forest.fit(X_train, y_train)\n",
    "cross_val_score(random_forest, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a713f7a",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8370177",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = random_forest.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433d1c9",
   "metadata": {},
   "source": [
    "# Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d30af",
   "metadata": {},
   "source": [
    "I used AurÃ©lion GÃ©ron's *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* as a reference text during this project. In it, GÃ©ron emphasizes the importance of creating a test set prior to any modeling so that it can be used to honestly assess how well your final model generalizes. I initially failed to adhere to this rule and so had to generate a brand new test set by randomly resampling 25% of the data set after already fitting my models on a previously sampled 25%. (Although I generated a new test set, overfitting might still pose an issue since I had been modifying my target vector to improve performance on the previous test set, and some of the data from the previous test set is still present within the new test set.) Indeed, I was so euthusiastic about modeling that I didn't give every step of the machine learning process its due, specifically EDA. After starting over with a new test set, I decided to spend additional time on EDA *before* proceeding to modeling. Having learned these lessons, I know that I'll allot the appropriate amount of time to EDA and be more concious of data leakage in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f400aea",
   "metadata": {},
   "source": [
    "# Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ccb67",
   "metadata": {},
   "source": [
    "These projects were especially helpful in helping me create my own.\n",
    "\n",
    "- [Finding Anime Genre Based on Synopsis(Logistic Reg](https://www.kaggle.com/code/shsagar/finding-anime-genre-based-on-synopsis-logistic-reg)\n",
    "- [A Beginner's Guide to Sentiment Analysis with Python](https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6)\n",
    "\n",
    "I also relied on this [regular expression cheat sheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
