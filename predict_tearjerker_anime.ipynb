{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94c2286",
   "metadata": {},
   "source": [
    "# When Computers Cry: Predicting Tearjerker Anime Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0797fa3",
   "metadata": {},
   "source": [
    "One of the few pieces of media to have ever coaxed tears from me is Kyoto Animation's devastating 2008 anime series *Clannad: After Story*, a show that I enthusiatically recommend. I do so not in spite of the fact that it crushed my soul, mind you, but because of it. If anything, I consider an anime's ability to turn on the waterworks a marker of quality. Most of my favorite anime are considered tearjerkers (In addition to *Clannad: After Story*, *A Place Further than the Universe* and *Your Name* come to mind as notable examples.). Even if I do not tear up while watching them, a decent attempt on the part of an anime's creators garners enormous credit from me nonetheless. \n",
    "\n",
    "Given my affinity for the medium, it comes as no surprise that predicting tearjerker anime would be a fun project with which to advance my understanding of machine learning and scikit-learn. Using data scraped off of the website MyAnimeList, courtesy of [Marlesson](https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews?select=animes.csv) and [Azathoth](https://www.kaggle.com/datasets/azathoth42/myanimelist?select=AnimeList.csv), I ...\n",
    "\n",
    "1. Labeled each member of a set of 1690 anime, every one of which has at least 10 MyAnimeList reviews available, as a tearjerker or a non-tearjerker based on the simultaneous occurence within a single review of at least one word that connotes saddness and at least one word that conveys crying.\n",
    "2. Used the total number of MyAnimeList users who reported they have watched or are planning to watch a particular anime and the studios that created said anime along with the score, number of episodes, synopsis, genre, and source of the anime as well as whether or not it was a TV show to predict the anime's tearjerker status using logistic regression.\n",
    "\n",
    "My logistic regression model ultimately achieved an accuracy of 87% when it was evaluated using five-fold cross validation on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "# https://pub.towardsai.net/emoticon-and-emoji-in-text-mining-7392c49f596a\n",
    "# https://medium.com/geekculture/text-preprocessing-how-to-handle-emoji-emoticon-641bbfa6e9e7\n",
    "from emot.emo_unicode import EMOTICONS_EMO, EMOJI_UNICODE\n",
    "# https://www.kaggle.com/code/shsagar/finding-anime-genre-based-on-synopsis-logistic-reg\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd399243",
   "metadata": {},
   "source": [
    "# Wrangle Anime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews?select=animes.csv\n",
    "animes = pd.read_csv(\"data/animes.csv\") \n",
    "# https://www.kaggle.com/datasets/azathoth42/myanimelist?select=AnimeList.csv\n",
    "AnimeList = pd.read_csv(\"data/AnimeList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnimeList = AnimeList[[\"anime_id\", \n",
    "                       \"title_english\", \n",
    "                       \"title_synonyms\", \n",
    "                       \"type\", \n",
    "                       \"source\", \n",
    "                       \"producer\", \n",
    "                       \"licensor\", \n",
    "                       \"studio\"]]\n",
    "animes = animes.merge(AnimeList, how = \"left\", left_on = \"uid\", right_on = \"anime_id\")\n",
    "anime = animes[[\"uid\", \n",
    "                \"title\", \n",
    "                \"title_english\", \n",
    "                \"title_synonyms\",\n",
    "                \"score\",\n",
    "                \"members\",\n",
    "                \"type\",\n",
    "                \"episodes\",\n",
    "                \"synopsis\", \n",
    "                \"genre\", \n",
    "                \"source\", \n",
    "                \"studio\",\n",
    "                \"producer\", \n",
    "                \"licensor\"]]\n",
    "anime = anime.drop_duplicates()\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d55bdc",
   "metadata": {},
   "source": [
    "# What is a tearjerker anime?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff2e23",
   "metadata": {},
   "source": [
    "In order to predict tearjerker anime, I must define it. To construct this target vector, I used a dataset of anime reviews. Thus, the determination of whether an anime is a tearjerker or not is based upon reviewers' reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.read_csv(\"data/reviews.csv\")\n",
    "# reviews = reviews.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3335c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# total_rows = len(reviews)\n",
    "# max_rows = 10000\n",
    "# num_files = math.ceil(total_rows/max_rows)\n",
    "\n",
    "# start = 0\n",
    "# end = 9999\n",
    "\n",
    "# for i in range(1, num_files + 1):\n",
    "#     i = str(i)\n",
    "#     print(\"Writing file #\" + i)\n",
    "#     reviews.iloc[start:(end + 1), :].to_csv(\"data/reviews/reviews\" + i + \".csv\", index = False)\n",
    "#     start += 10000\n",
    "#     end += 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8840c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "reviews = pd.read_csv(\"data/reviews/reviews1.csv\")\n",
    "for i in range(2, 15):\n",
    "    i = str(i)\n",
    "    print(f\"Concatenating data file #{i}\")\n",
    "    addition = pd.read_csv(f\"data/reviews/reviews{i}.csv\")\n",
    "    reviews = pd.concat([reviews, addition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant fields\n",
    "reviews = reviews[[\"uid\", \"anime_uid\", \"link\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counts = reviews.groupby(\"anime_uid\")[\"anime_uid\"].count() \n",
    "keep_anime = review_counts[review_counts >= 10].index.tolist()\n",
    "reviews = reviews[reviews[\"anime_uid\"].isin(keep_anime)]\n",
    "reviews = reviews.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481d3f7",
   "metadata": {},
   "source": [
    "## Remove front and back matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba92f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = re.compile(r\"^[\\s\\w]*Enjoyment[\\s\\d]*|\\s*Helpful\\s*$\") # Assumes reviews never start with a number\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(filler, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea346fe",
   "metadata": {},
   "source": [
    "## Replace emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e197e",
   "metadata": {},
   "source": [
    "The reviews feature heavy use of emoticons. These symbols allow users to succintly communicate an emotional reaction through pictograms comprised of punctuation, letters, numbers, etc. Because they convey information on emotion, I want to retain them to help me determine whether an anime is a tearjerker or not. However, since they include punctuation, which will be removed from the text later on in the process of constructing the target vector, I opted to replace them with verbal descriptions.\n",
    "\n",
    "I used a dictionary of emoticons from the emot library ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba493fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://introtopython.org/dictionaries.html#General-Syntax\n",
    "emoticons = {}\n",
    "for symbol, meaning in EMOTICONS_EMO.items():\n",
    "    emoticons[symbol] = \"\".join([word.capitalize() for word in meaning.replace(\",\", \"\").split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395ea7",
   "metadata": {},
   "source": [
    "... and added a few missing emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons[\"(^â€”^)\"] = \"NormalLaugh\"\n",
    "emoticons[\"-_-â€œ\"] = \"Troubled\" \n",
    "emoticons[\":s)\"] = \"HappyFaceOrSmiley\" \n",
    "emoticons[\":S)\"] = \"HappyFaceOrSmiley\" \n",
    "# https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "emoticons[\">W<\"] = \"Troubled\"\n",
    "emoticons[\"-_-'\"] = \"Troubled\"\n",
    "# https://www.urbandictionary.com/define.php?term=%3E_%3E\n",
    "emoticons[\">_>\"] = \"RightSidewaysLook\"\n",
    "emoticons[\"<_<\"] = \"LeftSidewaysLook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5788f9a8",
   "metadata": {},
   "source": [
    "Since many long emoticons are simply short ones with additional characters, I sorted the emoticons in descending order according to their length so that, later on, the longer emoticons would be matched prior their shorter counterparts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecd1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "# https://www.w3schools.com/python/ref_func_sorted.asp\n",
    "emoticons = dict(sorted(emoticons.items(), key = lambda item: -len(item[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5895d",
   "metadata": {},
   "source": [
    "I then assembled a list of emoticons that feature in both the emot dictionary and the reviews. I managed to get 97 unique emoticons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pythonforbeginners.com/basics/list-comprehensions-in-python\n",
    "# https://www.geeksforgeeks.org/python-accessing-key-value-in-dictionary/\n",
    "# https://stackoverflow.com/questions/4202538/escape-special-characters-in-a-python-string\n",
    "pattern = \"\\s(\" + \"|\".join([re.escape(emoticon) for emoticon in emoticons]) + \")\\W?\"\n",
    "used_emoticons = reviews[\"text\"].str.extractall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f32092",
   "metadata": {},
   "source": [
    "Here, I converted the DataFrame of used emoticons into a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.digitalocean.com/community/tutorials/python-convert-numpy-array-to-list\n",
    "used_emoticons = used_emoticons.dropna().drop_duplicates().iloc[:, 0].tolist()\n",
    "# https://stackoverflow.com/questions/5352546/extract-subset-of-key-value-pairs-from-dictionary\n",
    "used_emoticons = {symbol: emoticons[symbol] for symbol in used_emoticons}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f858c4b",
   "metadata": {},
   "source": [
    "Again, I sorted the used emoticons according to their length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_emoticons = dict(sorted(used_emoticons.items(), key = lambda item: -len(item[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a26df",
   "metadata": {},
   "source": [
    "Finally, I looped through each used emoticon and replaced it with its respective definition from the emot dictionary. To reduce the likelihood of false-positives, I only match an emoticon if it is preceded by a white space and followed by a non-word character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef79d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, meaning in used_emoticons.items():\n",
    "    print(f\"Replacing {symbol} with {meaning}\")\n",
    "    reviews[\"text\"] = reviews[\"text\"].str.replace(\"(?<=\\s)\" + re.escape(symbol) + \"(?=\\W?)\", meaning, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.to_csv(\"data/intermediates/replace_emoticons.csv\", index = False)\n",
    "# reviews = pd.read_csv(\"data/intermediates/replace_emoticons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3df35",
   "metadata": {},
   "source": [
    "## Drop titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93733741",
   "metadata": {},
   "source": [
    "I used the appearance of key words such as \"cry\" and \"tears\" in user reviews to determine whether an anime is a tearjerker or not. Some anime include these key words in their titles, meaning users cannot help but mention them in their reviews. Thus, to ensure that I only count authentic appearances of these terms, I exclude the title of the anime being discussed from the text of each review. (*True Tears* is a particularly strange case because one of its characters' ability to cry is a central plot point. Thus, reviewers who are summarizing its story mention crying quite a lot even after its reviews have been scrubbed of mentions of its title.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.merge(\n",
    "    anime[[\"uid\", \"title\", \"title_english\", \"title_synonyms\"]], \n",
    "    how = \"left\", left_on = \"anime_uid\", right_on = \"uid\"\n",
    ")\n",
    "reviews = reviews.drop(columns = \"uid_y\").rename(columns = {\"uid_x\": \"uid\"})\n",
    "reviews[\"text\"] = reviews[\"text\"].str.lower()\n",
    "reviews[\"title\"] = reviews[\"title\"].str.lower()\n",
    "reviews[\"title_english\"] = reviews[\"title_english\"].str.lower()\n",
    "reviews[\"title_synonyms\"] = reviews[\"title_synonyms\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in [\"title\", \"title_english\", \"title_synonyms\"]:\n",
    "    for i in range(len(reviews)):\n",
    "        print(f\"Processing row #{str(i)} for {field}\")\n",
    "        title = reviews[field].iloc[i]\n",
    "        if not pd.isna(title):\n",
    "            unigrams = reviews[field].iloc[[i]].str.split().iloc[0]\n",
    "            # http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "            bigrams = [\" \".join(unigram) for unigram in list(zip(unigrams, unigrams[1:]))]\n",
    "            pattern = \"|\".join([re.escape(title)] + [re.escape(bigram) for bigram in bigrams])\n",
    "            reviews[\"text\"].iloc[[i]] = reviews[\"text\"].iloc[[i]].str.replace(pattern, \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530983fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = reviews.drop(columns = [\"title\", \"title_english\", \"title_synonyms\"])\n",
    "# reviews.to_csv(\"data/intermediates/drop_titles.csv\", index = False)\n",
    "# reviews = pd.read_csv(\"data/intermediates/drop_titles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b58543",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0992709",
   "metadata": {},
   "source": [
    "I turned the DataFrame of reviews that I have been modifying into a DataFrame of tokens where each row is one token from a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.strip()\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"\\\\\", \" \", regex = True)\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"/\", \" \")                                      \n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"â€˜\", \"'\").str.replace(\"â€™\", \"'\")\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(\"â€œ\", '\"').str.replace(\"â€\", '\"')\n",
    "# \"â€“\" is used as punctuation while \"-\" is used to create phrases\n",
    "pattern = \"[\" + string.punctuation.replace(\"'\", \"\").replace(\"-\", \"\") + \"â€“\" + \"â€¦\" + \"]\" \n",
    "pattern = pattern + r\"|(?<=\\s)'(?=\\w)|(?<=\\w)'(?=\\s)\"\n",
    "reviews[\"text\"] = reviews[\"text\"].str.replace(pattern, \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.split()\n",
    "reviews = reviews.explode(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.to_csv(\"data/intermediates/tokenize_text.csv\", index = False)\n",
    "# reviews = pd.read_csv(\"data/intermediates/tokenize_text.csv\", na_filter = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555994d",
   "metadata": {},
   "source": [
    "## Replace non-word characters except for emojis and select punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca51a32",
   "metadata": {},
   "source": [
    "To further cleanse the tokens, I dropped nearly all non-word characters that are not emojis from the emot emoji dictionary. (Now that emoticons have been replaced with verbal descriptions, I can do this safe in the knowledge that their meaning would be perserved.) For the same reason as emoticons, I want to be able to use emojis to create my target vector because they convey emotional information. \n",
    "\n",
    "To do all this, I first created a list of all the unique non-word characters present in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c24573",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = reviews[\"text\"].drop_duplicates()\n",
    "non_word = unique_tokens.str.extractall(r\"(\\W)\").drop_duplicates()\n",
    "non_word = non_word.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571ea57",
   "metadata": {},
   "source": [
    "Next, I reformatted the definitions of the emoji dictionary that I obtained from the emot library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a68511",
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = {}\n",
    "for meaning, symbol in EMOJI_UNICODE.items():\n",
    "    emojis[symbol] = meaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76a6804",
   "metadata": {},
   "source": [
    "I also created a regular expression pattern that separates all the characters that I intend to drop by a pipe, `|`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51976328/best-way-to-remove-xad-in-python\n",
    "# https://stackoverflow.com/questions/31522361/python-getting-rid-of-u200b-from-a-string-using-regular-expressions\n",
    "# https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "drop = [char for char in non_word if char not in emojis]\n",
    "drop.remove(\"'\")\n",
    "drop.remove(\"-\")\n",
    "drop = \"|\".join(drop) # Create regex pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62892f82",
   "metadata": {},
   "source": [
    "Using the pattern from above, I replaced the specified non-word characters with empty strings. Afterward, I dropped tokens that became empty strings (They had once been composed entirely of the non-word characters that I just dropped.) along with tokens that are comprised of consecutive hyphens. Reviewers use these latter tokens as horizontal lines to format their pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text\"] = reviews[\"text\"].str.replace(drop, \"\", regex = True)\n",
    "reviews = reviews[(reviews[\"text\"] != \"\") & ~(reviews[\"text\"].str.fullmatch(r\"(-+)\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf0568",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b0503",
   "metadata": {},
   "source": [
    "I dropped tokens that are stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e89917",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords = list(stopwords.words(\"English\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[~reviews[\"text\"].isin(stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fefbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews.to_csv(\"data/intermediates/cleaned_text.csv\", index = False)\n",
    "# reviews = pd.read_csv(\"data/intermediates/cleaned_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5f175",
   "metadata": {},
   "source": [
    "## Add target field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72585f6d",
   "metadata": {},
   "source": [
    "Here, I defined what I mean by a tearjerker anime. An anime is considered a tearjerker if at least one of its reviews feature at least one word from the `sad_words` list in addition to at least one from `cry_words`. This dual criteria serves as an indicator of whether an anime prompted its audience to weap from sadness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_words = [\"sad\",\n",
    "             \"saddest\",\n",
    "             \"emotion\",\n",
    "             \"emotions\",\n",
    "             \"emotional\",\n",
    "             \"emotionally\",\n",
    "             \"depressed\",\n",
    "             \"depressing\",\n",
    "             \"depressingly\",\n",
    "             \"tragic\",\n",
    "             \"tragedy\",\n",
    "             \"sentimental\"]\n",
    "\n",
    "cry_words = [\"cry\", \n",
    "             \"cried\", \n",
    "             \"crying\", \n",
    "             \"sob\", \n",
    "             \"sobbed\", \n",
    "             \"sobbing\", \n",
    "             \"bawl\", \n",
    "             \"bawled\", \n",
    "             \"bawling\", \n",
    "             \"tear\", \n",
    "             \"tears\", \n",
    "             \"teared\", \n",
    "             \"tearing\", # as in \"tearing up\"\n",
    "             \"sadorcrying\",\n",
    "             \"tearsofhappiness\",\n",
    "             \"sadofcrying\",\n",
    "             \"ðŸ˜­\"]\n",
    "\n",
    "reviews[\"sad\"] = reviews[\"text\"].isin(sad_words)\n",
    "reviews[\"cry\"] = reviews[\"text\"].isin(cry_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f1279",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_vote = reviews.groupby([\"anime_uid\", \"uid\"]).agg(\n",
    "    sad = pd.NamedAgg(column = \"sad\", aggfunc = \"sum\"),\n",
    "    cry = pd.NamedAgg(column = \"cry\", aggfunc = \"sum\")\n",
    ")\n",
    "cry_vote[\"cry\"] = (cry_vote[\"sad\"] > 0) & (cry_vote[\"cry\"] > 0)\n",
    "cry_vote = cry_vote[\"cry\"].reset_index()\n",
    "cry_vote = cry_vote.groupby(\"anime_uid\").agg(\n",
    "    cry = pd.NamedAgg(column = \"cry\", aggfunc = \"mean\"),\n",
    "    reviews = pd.NamedAgg(column = \"uid\", aggfunc = \"nunique\")\n",
    ")\n",
    "cry_vote[\"cry\"] = cry_vote[\"cry\"] > 0\n",
    "cry_vote = cry_vote.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c03e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "cry_vote[(cry_vote[\"anime_uid\"] == 30276) & (cry_vote[\"cry\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d68d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[reviews[\"uid\"] == 206604][\"link\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = cry_vote.merge(anime, how = \"left\", left_on = \"anime_uid\", right_on = \"uid\").drop(columns = \"anime_uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime.to_csv(\"data/intermediates/anime.csv\", index = False)\n",
    "# anime = pd.read_csv(\"data/intermediates/anime.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f99b4d",
   "metadata": {},
   "source": [
    "# Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(anime, test_size = 0.2, random_state = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c63ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set.to_csv(\"data/intermediates/train.csv\", index = False)\n",
    "# train_set = pd.read_csv(\"data/intermediates/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed377fdc",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce013475",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae8d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93588ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Proportion of tearjerker anime: {(train_set['cry']).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[train_set[\"cry\"]].sort_values(by = \"members\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3516d83",
   "metadata": {},
   "source": [
    "I was suprised to see *One Punch Man* on this list since, in my view, the superhero comedy was as far from a tearjerker as one could get. However, one reviewer with the username [taikuroki](https://myanimelist.net/reviews.php?id=206604) disagreed.\n",
    ">This anime is one for the books. It was so enjoyable it has gotten to the point where I am literally pre-ordering the English manga sets, and potential Blu-ray release of the anime. Itâ€™s a definite GOAT for me; on the top 5 of my list. It had me laughing, *crying*, yelling, and pretending all at once while watching. (emphasis added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e10933",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[~train_set[\"cry\"]].sort_values(by = \"members\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7467000",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f746ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 4))\n",
    "\n",
    "genres = train_set[[\"cry\", \"genre\"]].copy()\n",
    "genres[\"genre\"] = genres[\"genre\"].str.replace(r\"[\\[\\]']\", \"\", regex = True).str.split(\", \")\n",
    "genres = genres.explode(\"genre\")\n",
    "cry = genres[genres[\"cry\"]].groupby(\"genre\")[\"genre\"].count()\n",
    "cry = cry.rename(\"count\").reset_index().sort_values(by = \"count\").tail()\n",
    "no_cry = genres[~genres[\"cry\"]].groupby(\"genre\")[\"genre\"].count()\n",
    "no_cry = no_cry.rename(\"count\").reset_index().sort_values(by = \"count\").tail()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(cry[\"genre\"], cry[\"count\"])\n",
    "plt.title(\"Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(no_cry[\"genre\"], no_cry[\"count\"])\n",
    "plt.title(\"Non-Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/genres.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf71c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 4))\n",
    "\n",
    "cry = train_set[train_set[\"cry\"]][[\"source\"]].groupby(\"source\")[\"source\"].count()\n",
    "cry = cry.rename(\"count\").reset_index().sort_values(by = \"count\").tail()\n",
    "\n",
    "no_cry = train_set[~train_set[\"cry\"]][[\"source\"]].groupby(\"source\")[\"source\"].count()\n",
    "no_cry = no_cry.rename(\"count\").reset_index().sort_values(by = \"count\").tail()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(cry[\"source\"], cry[\"count\"])\n",
    "plt.title(\"Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(no_cry[\"source\"], no_cry[\"count\"])\n",
    "plt.title(\"Non-Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/sources.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 4))\n",
    "\n",
    "types = train_set[[\"cry\", \"type\"]]\n",
    "cry = types[types[\"cry\"]][[\"type\"]].groupby(\"type\")[\"type\"].count().sort_values()\n",
    "no_cry = types[~types[\"cry\"]][[\"type\"]].groupby(\"type\")[\"type\"].count().sort_values()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(cry.index, cry.values)\n",
    "plt.title(\"Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(no_cry.index, no_cry.values)\n",
    "plt.title(\"Non-Tearjerker\")\n",
    "plt.xlabel(\"Animes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/types.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "plot_num = 1\n",
    "for var in [\"score\", \"members\", \"episodes\", \"reviews\"]:\n",
    "    plt.subplot(2, 2, plot_num)\n",
    "    cry = train_set[train_set[\"cry\"]][var].dropna()\n",
    "    no_cry = train_set[~train_set[\"cry\"]][var].dropna()\n",
    "    plt.boxplot([cry, no_cry], vert = False, widths = 0.6,\n",
    "                showfliers = False, labels = [\"Tearjerker\", \"Non-Tearjerker\"])\n",
    "    plt.xlabel(var.capitalize())\n",
    "    plot_num += 1\n",
    "\n",
    "# https://www.geeksforgeeks.org/how-to-set-the-spacing-between-subplots-in-matplotlib-in-python/\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/numeric.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30f777",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca90a4",
   "metadata": {},
   "source": [
    "## Select desired rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8653aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set[[\"cry\", \"score\", \"members\", \"type\", \"episodes\", \"synopsis\", \"genre\", \"source\", \"studio\"]]\n",
    "train_set = train_set.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[\"cry\"]\n",
    "X_train = train_set[[\"score\", \"members\", \"type\", \"episodes\", \"synopsis\", \"genre\", \"source\", \"studio\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee976b",
   "metadata": {},
   "source": [
    "## Clean `genre`, `studio`, and `synopsis` for `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca55720",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"genre\"] = X_train[\"genre\"].str.replace(\"[\\[\\]']\", \"\", regex = True).str.split(\", \")\n",
    "X_train[\"genre\"] = X_train[\"genre\"].apply(lambda genres: [genre.replace(\" \", \"\") for genre in genres])\n",
    "X_train[\"genre\"] = X_train[\"genre\"].apply(lambda genres: \" \".join(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"studio\"] = X_train[\"studio\"].str.split(\", \")\n",
    "X_train[\"studio\"] = X_train[\"studio\"].apply(lambda studios: [studio.replace(\" \", \"\") for studio in studios])\n",
    "X_train[\"studio\"] = X_train[\"studio\"].apply(lambda studios: \" \".join(studios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30811575",
   "metadata": {},
   "outputs": [],
   "source": [
    "filler = re.compile(r\"(?<=\\n)[^\\n]*$\")\n",
    "has_filler = X_train[\"synopsis\"].str.extractall(\"(.)$\").iloc[:, 0].str.match(\"[\\]\\)]\").reset_index(drop = True)\n",
    "X_train[\"synopsis\"][has_filler] = X_train[\"synopsis\"][has_filler].str.replace(filler, \"\").str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282cce2f",
   "metadata": {},
   "source": [
    "## Build data transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b976f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    (\"normalized\", StandardScaler(), [\"score\", \"members\", \"episodes\"]),\n",
    "    # https://stackoverflow.com/questions/65242617/sklearn-pipeline-with-countvectorizer-and-category-on-a-pandas-dataframe\n",
    "    # https://stackoverflow.com/questions/58772181/columntransformer-fails-with-countvectorizer-in-a-pipeline\n",
    "    (\"synopsis\", CountVectorizer(stop_words = stopwords), \"synopsis\"),\n",
    "    (\"genre\", CountVectorizer(stop_words = None), \"genre\"),\n",
    "    (\"studio\", CountVectorizer(stop_words = None), \"studio\"),\n",
    "    # https://www.learndatasci.com/glossary/dummy-variable-trap/\n",
    "    (\"one_hot\", OneHotEncoder(drop = \"first\"), [\"type\", \"source\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/23838056/what-is-the-difference-between-transform-and-fit-transform-in-sklearn\n",
    "X_train = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ec865",
   "metadata": {},
   "source": [
    "## Select model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "models = {\"Logistic Regression\": logistic_reg,\n",
    "          \"Decision Tree\": decision_tree, \n",
    "          \"Random Forest\": random_forest}.items()\n",
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X_train, y_train)\n",
    "    accuracy = np.round(np.mean(scores), decimals = 4)\n",
    "    std_dev = np.round(np.std(scores), decimals = 4)\n",
    "    print(f\"{name}: {str(accuracy)} ({str(std_dev)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f454f4d",
   "metadata": {},
   "source": [
    "As mentioned above, I opted for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4addf643",
   "metadata": {},
   "source": [
    "## Feature selection and parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ccbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/477486/how-do-i-use-a-decimal-step-value-for-range\n",
    "c_values = []\n",
    "thresholds = []\n",
    "accuracies = []\n",
    "# https://www.knime.com/blog/regularization-for-logistic-regression-l1-l2-gauss-or-laplace\n",
    "# https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\n",
    "# https://developers.google.com/machine-learning/glossary#l2-regularization\n",
    "inv_regs = np.linspace(1, 0.9, 11)\n",
    "multiples = np.linspace(0.5, 3, 251)\n",
    "\n",
    "for inv_reg in inv_regs:\n",
    "    logistic_reg_2 = LogisticRegression(C = inv_reg)\n",
    "    logistic_reg_2.fit(X_train, y_train)\n",
    "    for multiple in multiples:\n",
    "        print(f\"Fitting model where C = {inv_reg} and threshold = {multiple}\")\n",
    "        threshold = np.std(logistic_reg_2.coef_[0]) * multiple\n",
    "        selector = SelectFromModel(logistic_reg_2, threshold = threshold, prefit = True)\n",
    "        X_train_2 = selector.transform(X_train)\n",
    "        scores = cross_val_score(logistic_reg_2, X_train_2, y_train)\n",
    "        c_values.append(inv_reg)\n",
    "        thresholds.append(multiple)\n",
    "        accuracies.append(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c264b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\"c_value\": c_values, \"threshold\": thresholds, \"accuracy\": accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb36106",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = models[np.abs(models[\"c_value\"] - 0.96) < 1e-6]\n",
    "subset2 = models[np.abs(models[\"c_value\"] - 1) < 1e-6]\n",
    "max_score = models[\"accuracy\"].max()\n",
    "max_multiple = models[models[\"accuracy\"] == max_score][\"threshold\"].values\n",
    "\n",
    "# plt.figure(figsize = (12, 5.5))\n",
    "plt.axvline(x = max_multiple, color = \"red\", linestyle = \"--\", alpha = 0.5)\n",
    "plt.plot(subset[\"threshold\"], subset[\"accuracy\"], color = \"black\", label = \"Regularization parameter = 0.96\")\n",
    "plt.plot(subset2[\"threshold\"], subset2[\"accuracy\"], color = \"orange\", label = \"Regularization parameter = 1\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.xlabel(\"Threshold standard deviation\")\n",
    "plt.ylabel(\"Average accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/feature_selection.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ed0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[models[\"accuracy\"] == max_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e520353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://johaupt.github.io/blog/columnTransformer_feature_names.html\n",
    "\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"Get feature names from all transformers.\n",
    "    Returns\n",
    "    -------\n",
    "    feature_names : list of strings\n",
    "        Names of the features produced by transform.\n",
    "    \"\"\"\n",
    "    # Remove the internal helper function\n",
    "    #check_is_fitted(column_transformer)\n",
    "    \n",
    "    # Turn loopkup into function for better handling with pipeline later\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == 'drop' or (\n",
    "                hasattr(column, '__len__') and not len(column)):\n",
    "            return []\n",
    "        if trans == 'passthrough':\n",
    "            if hasattr(column_transformer, '_df_columns'):\n",
    "                if ((not isinstance(column, slice))\n",
    "                        and all(isinstance(col, str) for col in column)):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return ['x%d' % i for i in indices[column]]\n",
    "        if not hasattr(trans, 'get_feature_names'):\n",
    "        # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\"Transformer %s (type %s) does not \"\n",
    "                                 \"provide get_feature_names. \"\n",
    "                                 \"Will return input column names if available\"\n",
    "                                 % (str(name), type(trans).__name__))\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [name + \"__\" + f for f in column]\n",
    "\n",
    "        return [name + \"__\" + f for f in trans.get_feature_names()]\n",
    "    \n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == sklearn.pipeline.Pipeline:\n",
    "        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = list(column_transformer._iter(fitted=True))\n",
    "    \n",
    "    \n",
    "    for name, trans, column, _ in l_transformers: \n",
    "        if type(trans) == sklearn.pipeline.Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names)==0:\n",
    "                _names = [name + \"__\" + f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_final = LogisticRegression(C = 0.96)\n",
    "logistic_reg_final.fit(X_train, y_train)\n",
    "coefs = pd.DataFrame(\n",
    "    {\"feature_names\": get_feature_names(pipeline),\n",
    "     \"coef\": logistic_reg_final.coef_[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.sort_values(by = \"coef\", ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e249f",
   "metadata": {},
   "source": [
    "The most important predictors of tearjerker anime are the number of members who have watched or are planning to watch it as well as whether or not it is a drama. This finding coinsides with the visualizations from EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.std(coefs[\"coef\"].values) * 1.88\n",
    "\n",
    "# plt.figure(figsize = (8, 5.5))\n",
    "coefs[\"coef\"].hist(bins = 100, edgecolor = \"black\")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.axvline(x = threshold, color = \"red\", linestyle = \"--\")\n",
    "plt.axvline(x = -threshold, color = \"red\", linestyle = \"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/coef_hist.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bba56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(logistic_reg_final, threshold = threshold, prefit = True)\n",
    "X_train_final = selector.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db335c",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefc6c4",
   "metadata": {},
   "source": [
    "## With training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52048d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "train_pred = cross_val_predict(logistic_reg_final, X_train_final, y_train, cv = 5)\n",
    "print(confusion_matrix(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128623cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "train_pred_proba = cross_val_predict(logistic_reg_final, X_train_final, y_train, method = \"predict_proba\")\n",
    "fpr, tpr, thresholds = roc_curve(y_train, train_pred_proba[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], color = \"black\", linestyle = \"--\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/roc_train.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb55cce",
   "metadata": {},
   "source": [
    "## With test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042b592",
   "metadata": {},
   "source": [
    "# Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de436fe",
   "metadata": {},
   "source": [
    "# Note on potential overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b58ce",
   "metadata": {},
   "source": [
    "I used AurÃ©lion GÃ©ron's *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* as a reference text during this project. In it, GÃ©ron emphasizes the importance of creating a test set prior to any modeling so that it can be used to honestly assess how well your final model generalizes. I initially failed to adhere to this rule and so had to generate a brand new test set by randomly resampling 20% of the data set after already fitting my models on a previously sampled 25%. (Although I generated a new test set, overfitting might still pose an issue since I had been modifying my target vector to improve performance on the previous test set, and some of the data from the previous test set is still present within the new test set.) Having learned this lesson, I know that I'll be more concious of data leakage in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f400aea",
   "metadata": {},
   "source": [
    "# Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ccb67",
   "metadata": {},
   "source": [
    "For this project, I relied on the book *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*, video lectures from Statistical Science 663 at Duke University, and the LinkedIn Learning course [Python for Data Visualization](https://www.linkedin.com/learning/python-for-data-visualization).\n",
    "\n",
    "These projects were especially helpful in helping me create my own.\n",
    "\n",
    "- [Finding Anime Genre Based on Synopsis(Logistic Reg](https://www.kaggle.com/code/shsagar/finding-anime-genre-based-on-synopsis-logistic-reg)\n",
    "- [A Beginner's Guide to Sentiment Analysis with Python](https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6)\n",
    "\n",
    "I also used this [regular expression cheat sheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4530f5",
   "metadata": {},
   "source": [
    "# Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ef1bc",
   "metadata": {},
   "source": [
    "Marlesson. (2020). *Anime Dataset with Reviews - MyAnimeList* [Data set]. Kaggle. https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews\n",
    "\n",
    "MatÄ›j RaÄinskÃ½. (2018). <i>MyAnimeList Dataset</i> [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DSV/45582"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
